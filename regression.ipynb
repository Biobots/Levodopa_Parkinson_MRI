{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dl.loader import BlockRegressDataset\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = True\n",
    "writer = SummaryWriter('./log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data import getPandas, writePandas\n",
    "data = getPandas('data')\n",
    "test = data.sample(frac=0.2, random_state=10)\n",
    "data = data.drop(test.index)\n",
    "validate = data.sample(frac=0.1, random_state=10)\n",
    "train = data.drop(validate.index)\n",
    "test = test.reset_index(drop=True)\n",
    "train = train.reset_index(drop=True)\n",
    "validate = validate.reset_index(drop=True)\n",
    "writePandas('data_test', test)\n",
    "writePandas('data_train', train)\n",
    "writePandas('data_validate', validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dl.loader import BlockRegressDataset, RegressDataset\n",
    "from src.dl.resnet import RegressResNet3d\n",
    "from sklearn.model_selection import KFold\n",
    "import collections\n",
    "train_set = RegressDataset('train')\n",
    "val_set = RegressDataset('validate')\n",
    "test_set = RegressDataset('test')\n",
    "fold_num = 5\n",
    "kf = KFold(n_splits=fold_num, shuffle=True, random_state=10)\n",
    "best_models = np.empty(dtype=collections.OrderedDict, shape=fold_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0, loss 0.7013780163145757, nmse 0.32174277005611723, rmse 0.23821579993914418, mae 0.19104588079244306, rsquared -0.06789513135618752\n",
      "validation epoch 0, loss 0.054754057149350555, nmse 0.3517930346327366, rmse 0.19182213765722647, mae 0.15817149899947633, rsquared 0.0409262716685066\n",
      "train epoch 1, loss 0.6961282372062476, nmse 0.3193726368158819, rmse 0.23733676413791444, mae 0.18991510374005402, rsquared -0.06002843167099692\n",
      "validation epoch 1, loss 0.05724480492310152, nmse 0.3559610597379185, rmse 0.19295514082510887, mae 0.15361143667357924, rsquared 0.029563217304512945\n",
      "train epoch 2, loss 0.6924276597135457, nmse 0.31774001310463135, rmse 0.2367293575343636, mae 0.18960644935562554, rsquared -0.05460959689103606\n",
      "validation epoch 2, loss 0.06874913033622179, nmse 0.4208015431876771, rmse 0.20979434140464173, mae 0.16450759047371666, rsquared -0.14720777611182356\n",
      "train epoch 3, loss 0.6891127372471969, nmse 0.3162792609948871, rmse 0.23618457051991715, mae 0.1893058261765133, rsquared -0.04976122045093123\n",
      "validation epoch 3, loss 0.07070267022699861, nmse 0.43174533816730654, rmse 0.2125048941929716, mae 0.16587083539739497, rsquared -0.1770432339518746\n",
      "train epoch 4, loss 0.6861692049044279, nmse 0.3149842527381799, rmse 0.23570054463374496, mae 0.18903631302174606, rsquared -0.04546296376543513\n",
      "validation epoch 4, loss 0.07091721820365843, nmse 0.43243705991084413, rmse 0.21267505865481634, mae 0.16584040722623714, rsquared -0.1789290363590632\n",
      "train epoch 5, loss 0.6835546028252523, nmse 0.31383588362569237, rmse 0.23527049396164443, mae 0.188781202069703, rsquared -0.04165141647251103\n",
      "validation epoch 5, loss 0.0710150734437777, nmse 0.43246839117132513, rmse 0.2126827629632436, mae 0.16572605842453758, rsquared -0.1790144530269464\n",
      "train epoch 6, loss 0.6812308477220069, nmse 0.31281702676991197, rmse 0.23488828492391647, mae 0.18853965178882726, rsquared -0.03826973278884349\n",
      "validation epoch 6, loss 0.07110679311542348, nmse 0.43250047881674675, rmse 0.21269065297629536, mae 0.1655987618684116, rsquared -0.1791019317849043\n",
      "train epoch 7, loss 0.6791643900015677, nmse 0.3119125940452035, rmse 0.23454847856657318, mae 0.188310883324611, rsquared -0.035267833777447244\n",
      "validation epoch 7, loss 0.07120780284739263, nmse 0.4326272967638594, rmse 0.21272183330595246, mae 0.16548742851987727, rsquared -0.1794476684805828\n",
      "train epoch 8, loss 0.6773253535544546, nmse 0.3111091717509287, rmse 0.23424620962414464, mae 0.1881244773015702, rsquared -0.03260119807859563\n",
      "validation epoch 8, loss 0.0713114641413525, nmse 0.43280339897838754, rmse 0.21276512342524073, mae 0.1653835457519399, rsquared -0.1799277661255838\n",
      "train epoch 9, loss 0.6756874416019698, nmse 0.31039494946722723, rmse 0.23397717224203307, mae 0.18795007687612134, rsquared -0.0302306257753302\n",
      "validation epoch 9, loss 0.07141683625130954, nmse 0.4330168013443732, rmse 0.21281757100919657, mae 0.16528103930943638, rsquared -0.1805095530930101\n",
      "train epoch 10, loss 0.6742272828029312, nmse 0.3097594415265016, rmse 0.23373752507796255, mae 0.18778450223946536, rsquared -0.028121313930589364\n",
      "validation epoch 10, loss 0.07152432472266358, nmse 0.43327082485812846, rmse 0.21287998514744907, mae 0.1651846694360427, rsquared -0.1812020832298713\n",
      "train epoch 11, loss 0.6729242372497306, nmse 0.30919340159293257, rmse 0.23352386690874996, mae 0.1876272470593621, rsquared -0.026242573068421482\n",
      "validation epoch 11, loss 0.07163230155230334, nmse 0.4335532883762268, rmse 0.21294936557761474, mae 0.16512802141387412, rsquared -0.18197214776427306\n",
      "train epoch 12, loss 0.6717600765193098, nmse 0.30868865794115086, rmse 0.2333331807035528, mae 0.18747783357693204, rsquared -0.02456728044808809\n",
      "validation epoch 12, loss 0.07173986820550493, nmse 0.4338570036010093, rmse 0.21302394077210532, mae 0.16524220223884747, rsquared -0.1828001496412499\n",
      "train epoch 13, loss 0.6707186474527893, nmse 0.3082379861103052, rmse 0.233162790412049, mae 0.18733580769080932, rsquared -0.02307145739069494\n",
      "validation epoch 13, loss 0.07184632792194315, nmse 0.43417612859106336, rmse 0.21310227161158624, mae 0.1653488078553303, rsquared -0.18367016230177446\n",
      "train epoch 14, loss 0.669785639996812, nmse 0.30783498626554084, rmse 0.23301031844929004, mae 0.18720074049334393, rsquared -0.021733862230169265\n",
      "validation epoch 14, loss 0.07195107067575914, nmse 0.43450547560734065, rmse 0.21318308131990593, mae 0.16544829071676245, rsquared -0.1845680427022367\n",
      "train epoch 15, loss 0.6689484025650398, nmse 0.3074740059976682, rmse 0.2328736595187775, mae 0.18707222704163987, rsquared -0.020535734077951506\n",
      "validation epoch 15, loss 0.07205361551683194, nmse 0.43484073055618694, rmse 0.21326530916977812, mae 0.16554105992775128, rsquared -0.1854820295697348\n",
      "train epoch 16, loss 0.6681957529099498, nmse 0.30715006099038733, rmse 0.2327509529839505, mae 0.18694988912271926, rsquared -0.019460529509898183\n",
      "validation epoch 16, loss 0.0721535802134654, nmse 0.4351783094470939, rmse 0.21334807498975986, mae 0.16562754355801834, rsquared -0.1864023520708502\n",
      "train epoch 17, loss 0.667517846636369, nmse 0.30685875878968877, rmse 0.2326405558903073, mae 0.18683337341007952, rsquared -0.018493669549610292\n",
      "validation epoch 17, loss 0.07225065046302942, nmse 0.4355151045993081, rmse 0.21343061667290464, mae 0.16570810194473432, rsquared -0.18732053790888203\n",
      "train epoch 18, loss 0.666905931328433, nmse 0.30659620196896425, rmse 0.2325410077524522, mae 0.18672234562612527, rsquared -0.017622218264140566\n",
      "validation epoch 18, loss 0.07234459328854753, nmse 0.4358486168015969, rmse 0.21351232232667192, mae 0.1657830954250699, rsquared -0.1882297736236458\n",
      "train epoch 19, loss 0.6663522251187973, nmse 0.3063589398786585, rmse 0.23245101341557642, mae 0.18661648193943586, rsquared -0.016834722616456688\n",
      "validation epoch 19, loss 0.07243524786333397, nmse 0.43617682900545895, rmse 0.2135926990438785, mae 0.16585286537108154, rsquared -0.18912456024831692\n",
      "train epoch 20, loss 0.6658499464994557, nmse 0.30614395190917637, rmse 0.2323694376309051, mae 0.1865154898571153, rsquared -0.016121156913426082\n",
      "validation epoch 20, loss 0.07252248096657665, nmse 0.4364979709024932, rmse 0.2136713150304259, mae 0.16591770980599307, rsquared -0.19000007148984466\n",
      "train epoch 21, loss 0.6653930418722447, nmse 0.30594855944584565, rmse 0.2322952723838703, mae 0.1864190867179616, rsquared -0.015472630575885882\n",
      "validation epoch 21, loss 0.0726062303658889, nmse 0.43681080808352296, rmse 0.21374787020800748, mae 0.16597795926465286, rsquared -0.19085294204734193\n",
      "train epoch 22, loss 0.6649761799746124, nmse 0.30577040256518284, rmse 0.23222762861448218, mae 0.18632701106036284, rsquared -0.014881310791313052\n",
      "validation epoch 22, loss 0.07268643901016789, nmse 0.4371141483353447, rmse 0.21382207519429583, mae 0.16603385758424405, rsquared -0.19167992165645042\n",
      "train epoch 23, loss 0.6645946570615808, nmse 0.30560739910878654, rmse 0.23216572113086573, mae 0.18624315394790217, rsquared -0.014340286676149727\n",
      "validation epoch 23, loss 0.0727631055045628, nmse 0.437407223299281, rmse 0.21389374457027563, mae 0.16608567569497013, rsquared -0.19247891558376473\n",
      "train epoch 24, loss 0.6642442449640212, nmse 0.3054576881442687, rmse 0.23210884748846491, mae 0.18617150332731386, rsquared -0.013843381617206774\n",
      "validation epoch 24, loss 0.07283625353486743, nmse 0.43768946135268916, rmse 0.2139627411662693, mae 0.16613368452703467, rsquared -0.19324836544635726\n",
      "train epoch 25, loss 0.6639213506678386, nmse 0.30531968287841904, rmse 0.23205640837284766, mae 0.1861027330858223, rsquared -0.01338532889553501\n",
      "validation epoch 25, loss 0.07290591838024565, nmse 0.4379604046101161, rmse 0.21402895568406877, mae 0.1661780872780903, rsquared -0.19398702293665715\n",
      "train epoch 26, loss 0.6636226416043671, nmse 0.3051919264012685, rmse 0.23200785301783505, mae 0.18603666809637837, rsquared -0.01296129288712855\n",
      "validation epoch 26, loss 0.07297215263580721, nmse 0.4382197616505789, rmse 0.21409231951813146, mae 0.16621912236671613, rsquared -0.19469409356989287\n",
      "train epoch 27, loss 0.6633453123447659, nmse 0.3050731824289814, rmse 0.231962713857936, mae 0.18597315965461064, rsquared -0.012567170247290482\n",
      "validation epoch 27, loss 0.07303502865842076, nmse 0.43846738359530746, rmse 0.21415279884175267, mae 0.16625697402545053, rsquared -0.1953691714661776\n",
      "train epoch 28, loss 0.6630867942865476, nmse 0.3049623291041042, rmse 0.23192056630798713, mae 0.18591206427923498, rsquared -0.012199237423468734\n",
      "validation epoch 28, loss 0.0730946269757406, nmse 0.43870320971160115, rmse 0.2142103812708294, mae 0.16629184274264414, rsquared -0.19601209105333717\n",
      "train epoch 29, loss 0.6628448924492776, nmse 0.30485840892584426, rmse 0.2318810478530011, mae 0.18585324616997828, rsquared -0.011854316378641094\n",
      "validation epoch 29, loss 0.07315104334008346, nmse 0.4389273346927871, rmse 0.2142650922534976, mae 0.16632392900664755, rsquared -0.1966231104885079\n",
      "train epoch 30, loss 0.6626175552623894, nmse 0.3047605306452403, rmse 0.23184382083283608, mae 0.1857965689120865, rsquared -0.011529448971972611\n",
      "validation epoch 30, loss 0.07320438211116181, nmse 0.4391399020202352, rmse 0.21431696900101688, mae 0.16635337911977066, rsquared -0.19720262093694507\n",
      "train epoch 31, loss 0.6624031107143026, nmse 0.3046679698684459, rmse 0.23180861077173517, mae 0.18574191790487665, rsquared -0.011222230870766792\n",
      "validation epoch 31, loss 0.07325473575259589, nmse 0.4393410413280832, rmse 0.21436604519700322, mae 0.16638036647734375, rsquared -0.19775097581296808\n",
      "train epoch 32, loss 0.6621999927197711, nmse 0.30458004367962976, rmse 0.23177515875108268, mae 0.18568917731870396, rsquared -0.01093039541184071\n",
      "validation epoch 32, loss 0.07330221548024947, nmse 0.4395310177391239, rmse 0.21441238743070687, mae 0.16640503467237464, rsquared -0.19826889790605895\n",
      "train epoch 33, loss 0.6620068691621904, nmse 0.3044961808288135, rmse 0.23174324815136732, mae 0.18563824084837727, rsquared -0.010652046561693496\n",
      "validation epoch 33, loss 0.07334693842597056, nmse 0.4397101364905826, rmse 0.21445607190987656, mae 0.1664275408443814, rsquared -0.19875721936744029\n",
      "train epoch 34, loss 0.6618225514550968, nmse 0.3044158724144625, rmse 0.23171268592883357, mae 0.18558901048419718, rsquared -0.010385495227292374\n",
      "validation epoch 34, loss 0.07338899226542493, nmse 0.4398785935680522, rmse 0.21449714804239028, mae 0.16644798794684143, rsquared -0.199216474501702\n",
      "train epoch 35, loss 0.6616459171819561, nmse 0.30433863683395246, rmse 0.23168328930280999, mae 0.1855413743906446, rsquared -0.010129142923308132\n",
      "validation epoch 35, loss 0.07342851436180176, nmse 0.4400368735012999, rmse 0.21453573544041285, mae 0.16646653040997064, rsquared -0.19964798425532293\n",
      "train epoch 36, loss 0.6614761548310208, nmse 0.304264127612311, rmse 0.2316549268207558, mae 0.18549526773998734, rsquared -0.009881839666056491\n",
      "validation epoch 36, loss 0.07346560965780047, nmse 0.4401852973954067, rmse 0.21457191370928425, mae 0.1664833009895688, rsquared -0.20005262403917512\n",
      "train epoch 37, loss 0.6613123436550785, nmse 0.30419195172789915, rmse 0.2316274492291294, mae 0.18545200343206075, rsquared -0.009642280978999551\n",
      "validation epoch 37, loss 0.073500375684509, nmse 0.4403241524610188, rmse 0.21460575405795815, mae 0.16649836199958273, rsquared -0.20043117685962342\n",
      "train epoch 38, loss 0.661153810204763, nmse 0.3041218307223632, rmse 0.23160075081440934, mae 0.18541329144424087, rsquared -0.009409542632137446\n",
      "validation epoch 38, loss 0.07353292826923213, nmse 0.4404538717132415, rmse 0.21463736309846151, mae 0.16651184890511417, rsquared -0.2007848232215974\n",
      "train epoch 39, loss 0.6609999364176904, nmse 0.3040534997260343, rmse 0.23157473098043918, mae 0.18537880690048972, rsquared -0.009182745497620326\n",
      "validation epoch 39, loss 0.07356335905270991, nmse 0.4405747653342952, rmse 0.21466681738347215, mae 0.1665238511131303, rsquared -0.2011144087575829\n",
      "train epoch 40, loss 0.6608501509845434, nmse 0.30398672272084953, rmse 0.23154930006750443, mae 0.18534717096717407, rsquared -0.008961106208846381\n",
      "validation epoch 40, loss 0.07359178671911719, nmse 0.4406872834805087, rmse 0.21469422745354336, mae 0.1665344661585044, rsquared -0.20142116070367133\n",
      "train epoch 41, loss 0.6607039631847237, nmse 0.3039212936950761, rmse 0.23152437980078258, mae 0.18531613323098925, rsquared -0.008743940993103827\n",
      "validation epoch 41, loss 0.07361829781824955, nmse 0.44079174141038024, rmse 0.214719670876341, mae 0.16654377532029752, rsquared -0.20170593853152186\n",
      "train epoch 42, loss 0.6605609436011505, nmse 0.30385703539323305, rmse 0.2314999028222753, mae 0.18528566941994074, rsquared -0.00853066152244053\n",
      "validation epoch 42, loss 0.07364299548771283, nmse 0.4408885345459644, rmse 0.21474324464989006, mae 0.16655186800547678, rsquared -0.20196982025822674\n",
      "train epoch 43, loss 0.6604207400891688, nmse 0.3037938071962183, rmse 0.23147581570022155, mae 0.18525574942446568, rsquared -0.008320801068560701\n",
      "validation epoch 43, loss 0.07366597029891242, nmse 0.4409780322192009, rmse 0.21476503932586447, mae 0.1665588119465931, rsquared -0.20221381277285255\n",
      "train epoch 44, loss 0.6602829614694365, nmse 0.30373144690286663, rmse 0.23145205675538078, mae 0.18522633883363512, rsquared -0.008113821270231192\n",
      "validation epoch 44, loss 0.07368730869545084, nmse 0.44106055577978664, rmse 0.21478513369037833, mae 0.1665646830041035, rsquared -0.20243879215315452\n",
      "train epoch 45, loss 0.6601473844238803, nmse 0.3036698655247152, rmse 0.23142859217951384, mae 0.1851974152188972, rsquared -0.007909426766203653\n",
      "validation epoch 45, loss 0.07370711964487964, nmse 0.44113659682352774, rmse 0.21480364790757822, mae 0.16656955432916287, rsquared -0.20264609860935856\n",
      "train epoch 46, loss 0.6600137253900928, nmse 0.3036089491998727, rmse 0.2314053786704108, mae 0.18516895645894046, rsquared -0.007707239637933183\n",
      "validation epoch 46, loss 0.07372544028445444, nmse 0.4412062049054227, rmse 0.21482059445002977, mae 0.16657346385199973, rsquared -0.2028358672404933\n",
      "train epoch 47, loss 0.6598817778469673, nmse 0.3035486161893317, rmse 0.23138238515052945, mae 0.18514093459488512, rsquared -0.007506988585809893\n",
      "validation epoch 47, loss 0.07374239775907358, nmse 0.4412700109210443, rmse 0.21483612726725587, mae 0.1665765280726016, rsquared -0.20300981802196927\n",
      "train epoch 48, loss 0.6597513070750354, nmse 0.30348877499220983, rmse 0.23135957680838143, mae 0.18511332904162187, rsquared -0.007308369909623691\n",
      "validation epoch 48, loss 0.07375803986053214, nmse 0.4413281502090133, rmse 0.214850279609099, mae 0.16657873886306235, rsquared -0.20316831991991968\n",
      "train epoch 49, loss 0.6596221850925454, nmse 0.30342937677916393, rmse 0.23133693508480258, mae 0.18508612075024364, rsquared -0.007111221540099022\n",
      "validation epoch 49, loss 0.07377243940424857, nmse 0.44138092023249387, rmse 0.2148631241505147, mae 0.16658018563034915, rsquared -0.20331218389157701\n",
      "train epoch 50, loss 0.6594942339405342, nmse 0.3033703494448299, rmse 0.2313144325392004, mae 0.18505928329807286, rsquared -0.0069153041526117676\n",
      "validation epoch 50, loss 0.07378568883126967, nmse 0.4414287681230008, rmse 0.2148747699514104, mae 0.16658091172329462, rsquared -0.20344262892665044\n",
      "train epoch 51, loss 0.6593673217310896, nmse 0.30331164983410597, rmse 0.23129205275811152, mae 0.185032802244809, rsquared -0.006720474511238361\n",
      "validation epoch 51, loss 0.07379782683531222, nmse 0.44147179475285553, rmse 0.2148852417563557, mae 0.16658095236282514, rsquared -0.20355993002772377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m \u001b[39m#scheduler.step()\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m predy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((predy, output\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     28\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((y, score\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = RegressResNet3d().cuda()\n",
    "loss_fn = nn.SmoothL1Loss().cuda()\n",
    "#lr = 1e-2\n",
    "#optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "lr = 0.01\n",
    "optim = torch.optim.SGD([{'params': net.fc.weight, 'lr': 1e-2}], lr=lr)\n",
    "epoch = 100\n",
    "train_loader = DataLoader(train_set, batch_size=8)\n",
    "val_loader = DataLoader(val_set, batch_size=8)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    predy = np.array([])\n",
    "    y = np.array([])\n",
    "    net.train()\n",
    "    for step, [img, labels, score] in enumerate(train_loader):\n",
    "        img = img.cuda()\n",
    "        labels = labels.cuda()\n",
    "        score = score.cuda()\n",
    "        output = net(img, labels).squeeze(-1)\n",
    "        loss = loss_fn(output, score)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "        y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(train_set)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(train_set)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = r2_score(y, predy)\n",
    "    print('train epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))\n",
    "    \n",
    "    if log:\n",
    "        writer.add_scalar('train/Loss', total_loss, i)\n",
    "        writer.add_scalar('train/NMSE', nmse, i)\n",
    "        writer.add_scalar('train/RMSE', rmse, i)\n",
    "        writer.add_scalar('train/MAE', mae, i)\n",
    "        writer.add_scalar('train/rsquared', rsquared, i)\n",
    "        \n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    predy = np.array([])\n",
    "    y = np.array([])\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for step, [img, labels, score] in enumerate(val_loader):\n",
    "            img = img.cuda()\n",
    "            labels = labels.cuda()\n",
    "            score = score.cuda()\n",
    "            output = net(img, labels).squeeze(-1)\n",
    "            loss = loss_fn(output, score)\n",
    "\n",
    "            predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "            y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(val_set)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(val_set)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = r2_score(y, predy)\n",
    "    print('validation epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))\n",
    "    if log:\n",
    "        writer.add_scalar('validation/Loss', total_loss, i)\n",
    "        writer.add_scalar('validation/NMSE', nmse, i)\n",
    "        writer.add_scalar('validation/RMSE', rmse, i)\n",
    "        writer.add_scalar('validation/MAE', mae, i)\n",
    "        writer.add_scalar('validation/rsquared', rsquared, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss 11.664599132818774, nmse 0.2849210541321967, rmse 0.4760008485561512, mae 0.8189351787140201, rsquared -0.016067644564553785\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for step, [img, labels, score] in enumerate(test_loader):\n",
    "        img = img.cuda()\n",
    "        labels = labels.cuda()\n",
    "        score = score.cuda()\n",
    "        #score = torch.Tensor(np.random.randint(0, 3, score.size())).type(torch.LongTensor).cuda()\n",
    "        output = net(img, labels).squeeze(-1)\n",
    "        loss = loss_fn(output, score)\n",
    "        \n",
    "        predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "        y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "        total_loss += loss.item()\n",
    "nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "mse = np.sum((y - predy) ** 2) / len(test_set)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.sum(np.abs(y - predy)) / len(test_set)\n",
    "meany = np.mean(y)\n",
    "rsquared = 1 - np.sum((predy - y) ** 2) / np.sum((y - meany) ** 2)\n",
    "print('epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss().cuda()\n",
    "lr = 1e-3\n",
    "optim = torch.optim.SGD([\n",
    "    {'params': net.fc.weight, 'lr': 1e-2}\n",
    "    ], lr=lr, momentum=0.3)\n",
    "#optim = torch.optim.SGD(net.parameters(), lr=1e-1, momentum=0.3)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.99)\n",
    "#optim = torch.optim.Adam(net.parameters(), lr=lr, betas=[0.3, 0.1])\n",
    "epoch = 100\n",
    "dataset = BlockRegressDataset('train')\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "    train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = sampler.SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
    "    for i in range(epoch):\n",
    "        total_loss = 0\n",
    "        predy = np.array([])\n",
    "        y = np.array([])\n",
    "        net.train()\n",
    "        for step, [img, labels, score] in enumerate(train_loader):\n",
    "            img = img.cuda()\n",
    "            labels = labels.cuda()\n",
    "            score = score.cuda()\n",
    "            output = net(img, labels).squeeze(-1)\n",
    "            loss = loss_fn(output.float(), score.float())\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            #scheduler.step()\n",
    "        \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, [img, labels, score] in enumerate(val_loader):\n",
    "                img = img.cuda()\n",
    "                labels = labels.cuda()\n",
    "                score = score.cuda()\n",
    "                output = net(img, labels).squeeze(-1)\n",
    "                loss = loss_fn(output.float(), score.float())\n",
    "        \n",
    "                predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "                y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "        mse = np.sum((y - predy) ** 2) / len(train_idx)\n",
    "        mae = np.sum(np.abs(y - predy)) / len(train_idx)\n",
    "        meany = np.mean(y)\n",
    "        rsquared = np.sum((predy - meany) ** 2) / np.sum((y - meany) ** 2)\n",
    "        print('epoch {}, loss {}, nmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, mae, rsquared))\n",
    "        writer.add_scalar('Loss', total_loss, i)\n",
    "        writer.add_scalar('NMSE', nmse, i)\n",
    "        writer.add_scalar('MAE', mae, i)\n",
    "        writer.add_scalar('rsquared', rsquared, i)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predy = []\n",
    "y = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "    train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = sampler.SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, [img, labels, score] in enumerate(train_loader):\n",
    "            img = img.cuda()\n",
    "            labels = labels.cuda()\n",
    "            score = score.cuda()\n",
    "            output = net(img, labels).squeeze(-1)\n",
    "            loss = loss_fn(output.float(), score.float())\n",
    "    \n",
    "            predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "            y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(train_idx)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(train_idx)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = np.sum((predy - meany) ** 2) / np.sum((y - meany) ** 2)\n",
    "    print(rsquared)\n",
    "    print(mse)\n",
    "    print(mae)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "420353a47962bc0cada1a6173771095a3d05bd8a3ecc61a5b633bf029926f1ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
