{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = True\n",
    "writer = SummaryWriter('./log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data import getPandas, writePandas\n",
    "data = getPandas('data')\n",
    "test = data.sample(frac=0.2, random_state=10)\n",
    "data = data.drop(test.index)\n",
    "validate = data.sample(frac=0.1, random_state=10)\n",
    "train = data.drop(validate.index)\n",
    "test = test.reset_index(drop=True)\n",
    "train = train.reset_index(drop=True)\n",
    "validate = validate.reset_index(drop=True)\n",
    "writePandas('data_test', test)\n",
    "writePandas('data_train', train)\n",
    "writePandas('data_validate', validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0, loss 1.238169008374675, nmse 0.5576014266935511, rmse 0.3136014795915315, mae 0.24791561923659725, rsquared -0.8507326480077531\n",
      "validation epoch 0, loss 0.06367563467945775, nmse 0.42387602993288237, rmse 0.2105593529209412, mae 0.16465632709149858, rsquared -0.15558957783938787\n",
      "train epoch 1, loss 0.7165192718651713, nmse 0.31615727153171413, rmse 0.23613901772643772, mae 0.19001760296363374, rsquared -0.04935632571537463\n",
      "validation epoch 1, loss 0.07204838595883484, nmse 0.4797253264331429, rmse 0.22400176059708146, mae 0.17174906849421132, rsquared -0.3078483997774488\n",
      "train epoch 2, loss 0.7155078450978667, nmse 0.3157359133697811, rmse 0.2359816082891661, mae 0.18984858964080026, rsquared -0.04795779753832474\n",
      "validation epoch 2, loss 0.07229845382065427, nmse 0.4813739661114916, rmse 0.22438633634146174, mae 0.17194984944943925, rsquared -0.3123429941762257\n",
      "train epoch 3, loss 0.7146349724735706, nmse 0.31539373258584397, rmse 0.23585370034766948, mae 0.18974711903149913, rsquared -0.0468220666774688\n",
      "validation epoch 3, loss 0.07217727355232426, nmse 0.48056390381756564, rmse 0.22419745674111247, mae 0.17185152481462715, rsquared -0.3101345665272037\n",
      "train epoch 4, loss 0.7137156905143647, nmse 0.3150383598160655, rmse 0.23572078774112432, mae 0.18963666870594617, rsquared -0.04564255034957476\n",
      "validation epoch 4, loss 0.07203956030092545, nmse 0.4796360890778928, rmse 0.223980925492842, mae 0.17175236977874125, rsquared -0.30760511695322923\n",
      "train epoch 5, loss 0.7129209349660945, nmse 0.31469936369514534, rmse 0.23559393026796735, mae 0.18953120028407897, rsquared -0.04451738969090302\n",
      "validation epoch 5, loss 0.07209916489284088, nmse 0.48002825651476794, rmse 0.22407247414808956, mae 0.17179660590772172, rsquared -0.308674261162428\n",
      "train epoch 6, loss 0.7120307652032986, nmse 0.314352244134307, rmse 0.23546396205937656, mae 0.18943730914273807, rsquared -0.04336526655553907\n",
      "validation epoch 6, loss 0.07197214758878481, nmse 0.4791748040667969, rmse 0.22387319392352234, mae 0.17171226669001555, rsquared -0.3063475413566088\n",
      "train epoch 7, loss 0.7112578025680577, nmse 0.3140287093914782, rmse 0.23534275983250325, mae 0.18933614346957062, rsquared -0.04229142369457528\n",
      "validation epoch 7, loss 0.07195009090444004, nmse 0.4790254045254231, rmse 0.223838291045017, mae 0.17167234918847926, rsquared -0.3059402417200314\n",
      "train epoch 8, loss 0.7103332882732707, nmse 0.3136535466424696, rmse 0.23520213857733124, mae 0.18922930921034078, rsquared -0.04104622252637924\n",
      "validation epoch 8, loss 0.07181598113148675, nmse 0.47812734711781585, rmse 0.22362837113496162, mae 0.17156853279844173, rsquared -0.3034919179006903\n",
      "train epoch 9, loss 0.709555600353272, nmse 0.31334318671550115, rmse 0.23508574361548631, mae 0.18913213305424761, rsquared -0.04001610814363854\n",
      "validation epoch 9, loss 0.07181889251643887, nmse 0.4781345917465642, rmse 0.22363006534718277, mae 0.17156232036887492, rsquared -0.30351166852796263\n",
      "train epoch 10, loss 0.708692695119065, nmse 0.3129901341204463, rmse 0.23495326742406716, mae 0.18902914406270688, rsquared -0.038844292698318794\n",
      "validation epoch 10, loss 0.07167803866307847, nmse 0.47718944699132554, rmse 0.22340892746846092, mae 0.17143052366206837, rsquared -0.30093497309916883\n",
      "train epoch 11, loss 0.7079531208525606, nmse 0.31269596115601345, rmse 0.23484282760557465, mae 0.1889363403988927, rsquared -0.03786790439769061\n",
      "validation epoch 11, loss 0.07166453267838055, nmse 0.4770942921042693, rmse 0.22338665171270916, mae 0.17141917304119172, rsquared -0.3006755576380491\n",
      "train epoch 12, loss 0.7071059232181381, nmse 0.31236430254629566, rmse 0.23471825242859842, mae 0.18883788996308715, rsquared -0.036767097642877555\n",
      "validation epoch 12, loss 0.0715867767377775, nmse 0.47657149934781834, rmse 0.22326422629736864, mae 0.17136007097194386, rsquared -0.29925029690598937\n",
      "train epoch 13, loss 0.7064364313090274, nmse 0.31208903095980745, rmse 0.2346148066984522, mae 0.1887501780414149, rsquared -0.035853444829606396\n",
      "validation epoch 13, loss 0.07156990829156629, nmse 0.47646064632202967, rmse 0.22323825857326698, mae 0.17135420262373555, rsquared -0.2989480844848398\n",
      "train epoch 14, loss 0.705660797429401, nmse 0.31176551340284014, rmse 0.23449317203312214, mae 0.18865909528249503, rsquared -0.03477965901016633\n",
      "validation epoch 14, loss 0.07146286855157633, nmse 0.47573270109949334, rmse 0.22306765965587574, mae 0.17126741690629155, rsquared -0.29696352802729686\n",
      "train epoch 15, loss 0.7048847024359262, nmse 0.31145081363664673, rmse 0.23437479207283882, mae 0.18857705778445352, rsquared -0.03373514028454494\n",
      "validation epoch 15, loss 0.07144298336000016, nmse 0.47559253937251633, rmse 0.22303479681970387, mae 0.17123536179926155, rsquared -0.2965814129288511\n",
      "train epoch 16, loss 0.7041425172224789, nmse 0.3111619805706524, rmse 0.23426608968889215, mae 0.18849075947603186, rsquared -0.03277647561930275\n",
      "validation epoch 16, loss 0.0713239459221045, nmse 0.47479023291059547, rmse 0.222846591817718, mae 0.17112831050259392, rsquared -0.2943941295720267\n",
      "train epoch 17, loss 0.7034694660368326, nmse 0.31089282435664434, rmse 0.23416474727058495, mae 0.18840941603413747, rsquared -0.031883120314183166\n",
      "validation epoch 17, loss 0.071409086630047, nmse 0.4753556304203889, rmse 0.22297923925993474, mae 0.17121397456899531, rsquared -0.29593554126675525\n",
      "train epoch 18, loss 0.7027453312560747, nmse 0.3105905031816524, rmse 0.23405086529158314, mae 0.18832929418022057, rsquared -0.030879687320728433\n",
      "validation epoch 18, loss 0.07132532813431279, nmse 0.4747934724514888, rmse 0.22284735206863054, mae 0.17112636522373262, rsquared -0.2944029613516721\n",
      "train epoch 19, loss 0.7020482082903817, nmse 0.31030911926909804, rmse 0.23394482040195944, mae 0.188249047433066, rsquared -0.029945747110645904\n",
      "validation epoch 19, loss 0.07127936164584531, nmse 0.47447676175823805, rmse 0.22277301457542373, mae 0.17107373703169798, rsquared -0.29353953065385996\n",
      "train epoch 20, loss 0.7013347976130839, nmse 0.3100236938375382, rmse 0.23383720325659466, mae 0.18816602630476548, rsquared -0.028998392711121035\n",
      "validation epoch 20, loss 0.07122851315354556, nmse 0.47413569310346865, rmse 0.22269293209531918, mae 0.17102239033952601, rsquared -0.29260969420417515\n",
      "train epoch 21, loss 0.7006452680192155, nmse 0.30973455666672345, rmse 0.23372813611094398, mae 0.1880854288507374, rsquared -0.02803871869279484\n",
      "validation epoch 21, loss 0.07118278668874861, nmse 0.47382143132350946, rmse 0.222619118334209, mae 0.17100776281783772, rsquared -0.2917529398420764\n",
      "train epoch 22, loss 0.6999792354425515, nmse 0.30948899722702544, rmse 0.2336354671964996, mae 0.18801409299797675, rsquared -0.027223683346184524\n",
      "validation epoch 22, loss 0.07115746000836909, nmse 0.4736370856829997, rmse 0.22257580786566308, mae 0.1709464553767332, rsquared -0.29125036860461595\n",
      "train epoch 23, loss 0.6992829025161964, nmse 0.3091930799555518, rmse 0.23352374544751556, mae 0.18793243186947053, rsquared -0.02624150552308735\n",
      "validation epoch 23, loss 0.07111350217281912, nmse 0.4733259386367448, rmse 0.22250268734283543, mae 0.17090518593348134, rsquared -0.29040210576728454\n",
      "train epoch 24, loss 0.6986445887392296, nmse 0.30894923413896413, rmse 0.23343164277165612, mae 0.18786450024075171, rsquared -0.02543215785603614\n",
      "validation epoch 24, loss 0.07110341425501829, nmse 0.47327577556244077, rmse 0.22249089661506802, mae 0.1709053227532341, rsquared -0.2902653489757592\n",
      "train epoch 25, loss 0.6979413194209424, nmse 0.3086664303657946, rmse 0.23332477980495925, mae 0.18779062381681766, rsquared -0.02449350499230829\n",
      "validation epoch 25, loss 0.0710539990772703, nmse 0.47293777138192844, rmse 0.22241143312997189, mae 0.17086012211272908, rsquared -0.289343867031314\n",
      "train epoch 26, loss 0.6973219109061372, nmse 0.3084146099628742, rmse 0.23322958330327204, mae 0.18771932375756792, rsquared -0.02365768890790032\n",
      "validation epoch 26, loss 0.07103190210805514, nmse 0.4727845962871136, rmse 0.22237541290348178, mae 0.170834746789867, rsquared -0.28892627431397977\n",
      "train epoch 27, loss 0.6966484960157021, nmse 0.30814049817260564, rmse 0.2331259157272753, mae 0.18764542321525957, rsquared -0.02274788556958751\n",
      "validation epoch 27, loss 0.07100801237177468, nmse 0.47261352487299235, rmse 0.22233517733051733, mae 0.17080030614239494, rsquared -0.2884598918595247\n",
      "train epoch 28, loss 0.6959843911069445, nmse 0.3078773036915588, rmse 0.23302633361732425, mae 0.18757034440587533, rsquared -0.021874317828311618\n",
      "validation epoch 28, loss 0.07092052583937072, nmse 0.4720137438815279, rmse 0.2221940527737436, mae 0.17069916112416417, rsquared -0.28682474239652667\n",
      "train epoch 29, loss 0.6953636462545092, nmse 0.3076220664510363, rmse 0.23292972154214947, mae 0.1875011249788649, rsquared -0.02102716093198964\n",
      "validation epoch 29, loss 0.07093487769430508, nmse 0.4721135573387738, rmse 0.22221754444590755, mae 0.1707483010898284, rsquared -0.28709685825771314\n",
      "train epoch 30, loss 0.6947782055497363, nmse 0.3073897809781581, rmse 0.23284176229901626, mae 0.18743953970861246, rsquared -0.020256183155152208\n",
      "validation epoch 30, loss 0.07090279504355829, nmse 0.47189395859829353, rmse 0.22216585733802185, mae 0.1707119788320929, rsquared -0.28649817846011794\n",
      "train epoch 31, loss 0.6941536507618574, nmse 0.30714049458285836, rmse 0.23274732835858125, mae 0.18737082553850065, rsquared -0.01942877768522533\n",
      "validation epoch 31, loss 0.07085071942426487, nmse 0.4715452173586628, rmse 0.22208374914932646, mae 0.17066291472688563, rsquared -0.28554742467028005\n",
      "train epoch 32, loss 0.6935182539363545, nmse 0.3068900184395214, rmse 0.2326524051155764, mae 0.18729987865213324, rsquared -0.0185974233273829\n",
      "validation epoch 32, loss 0.07083906494216129, nmse 0.4714593487914064, rmse 0.22206352746019506, mae 0.17064728747274635, rsquared -0.2853133259848668\n",
      "train epoch 33, loss 0.6929014036463809, nmse 0.30663492762767297, rmse 0.23255569322360428, mae 0.18723056244438513, rsquared -0.017750752441882733\n",
      "validation epoch 33, loss 0.07080369677499332, nmse 0.47122244048090894, rmse 0.222007726990996, mae 0.1706045279133751, rsquared -0.28466745607201105\n",
      "train epoch 34, loss 0.6922779471394455, nmse 0.30639149672611504, rmse 0.23246336440350357, mae 0.1871610391565825, rsquared -0.016942781917634653\n",
      "validation epoch 34, loss 0.07079525321912548, nmse 0.47116008994158654, rmse 0.22199303885409477, mae 0.17059140811826942, rsquared -0.28449747327438724\n",
      "train epoch 35, loss 0.6916678283463514, nmse 0.3061447006725225, rmse 0.23236972179397639, mae 0.1870919928595583, rsquared -0.01612364213083084\n",
      "validation epoch 35, loss 0.0707541180164926, nmse 0.47087261987530715, rmse 0.22192530594119167, mae 0.17055005197735243, rsquared -0.28371375966693035\n",
      "train epoch 36, loss 0.6910405510862339, nmse 0.30589311742900244, rmse 0.23227422390771654, mae 0.1870209031063877, rsquared -0.015288613201232648\n",
      "validation epoch 36, loss 0.07073153675521603, nmse 0.4707144165973062, rmse 0.22188802169606692, mae 0.17052317028255873, rsquared -0.28328245889423287\n",
      "train epoch 37, loss 0.6904467346518643, nmse 0.30565354736509026, rmse 0.23218324956570868, mae 0.18695403913818742, rsquared -0.014493457167653823\n",
      "validation epoch 37, loss 0.07070355827009603, nmse 0.4705336319358832, rmse 0.22184540795641208, mae 0.1704903294779385, rsquared -0.28278959575543583\n",
      "train epoch 38, loss 0.6898376610726742, nmse 0.30541160751200114, rmse 0.23209133912924584, mae 0.18688811271418057, rsquared -0.013690435576368953\n",
      "validation epoch 38, loss 0.07068529660304015, nmse 0.47040113036039016, rmse 0.22181417008758186, mae 0.1704729113751539, rsquared -0.2824283641007199\n",
      "train epoch 39, loss 0.6892425482358535, nmse 0.3051697062591258, rmse 0.23199940695346638, mae 0.18681734310554532, rsquared -0.012887542102897775\n",
      "validation epoch 39, loss 0.0705876896332174, nmse 0.4697536481337021, rmse 0.22166145979511992, mae 0.17038956105269062, rsquared -0.2806631694207671\n",
      "train epoch 40, loss 0.6886933480045669, nmse 0.30494804912002266, rmse 0.23191513635738462, mae 0.18675579393604425, rsquared -0.012151840785857182\n",
      "validation epoch 40, loss 0.07065611201449512, nmse 0.47020568324687434, rmse 0.22176808448042712, mae 0.1704331916526662, rsquared -0.28189552753661085\n",
      "train epoch 41, loss 0.6880487094239507, nmse 0.3046898126653014, rmse 0.23181692023998554, mae 0.1866831861078745, rsquared -0.011294729209776033\n",
      "validation epoch 41, loss 0.07063864018287853, nmse 0.470074105014086, rmse 0.22173705349193773, mae 0.170406193457885, rsquared -0.2815368131396092\n",
      "train epoch 42, loss 0.6874412389449448, nmse 0.30444796150538017, rmse 0.23172489825772838, mae 0.18661462494350028, rsquared -0.010492001999627298\n",
      "validation epoch 42, loss 0.07061363489127277, nmse 0.4699019054517706, rmse 0.2216964359375607, mae 0.17037730146098112, rsquared -0.28106735507765723\n",
      "train epoch 43, loss 0.6868707695576048, nmse 0.3042147209109885, rmse 0.2316361178823972, mae 0.18654804133219874, rsquared -0.009717854082823774\n",
      "validation epoch 43, loss 0.07059542098217325, nmse 0.46977363705798797, rmse 0.22166617580639633, mae 0.17035335123098957, rsquared -0.2807176641101241\n",
      "train epoch 44, loss 0.6862403215290136, nmse 0.3039620124390156, rmse 0.23153988885919397, mae 0.18647587654473913, rsquared -0.008879090412003077\n",
      "validation epoch 44, loss 0.07058907936918282, nmse 0.4697318305927339, rmse 0.22165631224155277, mae 0.1703375871570975, rsquared -0.2806036894757442\n",
      "train epoch 45, loss 0.685655925745061, nmse 0.30372516550986783, rmse 0.2314496634423729, mae 0.18640715884136227, rsquared -0.008092972724045522\n",
      "validation epoch 45, loss 0.07056955092262028, nmse 0.46959858379211317, rmse 0.22162487186713795, mae 0.1703100687763255, rsquared -0.2802404261553293\n",
      "train epoch 46, loss 0.6850883918547123, nmse 0.3035010098777906, rmse 0.2313642402912764, mae 0.1863431981416713, rsquared -0.0073489786690443815\n",
      "validation epoch 46, loss 0.07054672914500731, nmse 0.46943697044884203, rmse 0.22158673225180328, mae 0.17028628923236216, rsquared -0.27979982871188946\n",
      "train epoch 47, loss 0.6845090134370031, nmse 0.3032691007855372, rmse 0.23127582917759157, mae 0.18627837870495892, rsquared -0.0065792501356218835\n",
      "validation epoch 47, loss 0.07045123858720803, nmse 0.46880400482656936, rmse 0.2214372935588262, mae 0.17020469047366465, rsquared -0.2780742098408606\n",
      "train epoch 48, loss 0.6839623881449356, nmse 0.3030483519603454, rmse 0.23119164130396283, mae 0.18621331716278222, rsquared -0.005846563599623433\n",
      "validation epoch 48, loss 0.07052768332959194, nmse 0.46930783651808927, rmse 0.22155625283109423, mae 0.17025803934004066, rsquared -0.2794477780791915\n",
      "train epoch 49, loss 0.6833504166685342, nmse 0.30280130172729247, rmse 0.23109738638767993, mae 0.18614144333201643, rsquared -0.005026580166797023\n",
      "validation epoch 49, loss 0.07052662303682364, nmse 0.4692922495862534, rmse 0.2215525735713244, mae 0.17024581767855967, rsquared -0.27940528429632083\n",
      "train epoch 50, loss 0.6827626118383617, nmse 0.30256539062056315, rmse 0.23100734538921133, mae 0.1860745050116304, rsquared -0.0042435685632569164\n",
      "validation epoch 50, loss 0.07050379212126492, nmse 0.46913583273141485, rmse 0.2215156483452265, mae 0.1702190687395657, rsquared -0.27897885374944975\n",
      "train epoch 51, loss 0.6821711979710385, nmse 0.30233847297139743, rmse 0.23092070382762733, mae 0.1860102968297674, rsquared -0.0034904071084693733\n",
      "validation epoch 51, loss 0.07038858853826285, nmse 0.4683739326580769, rmse 0.2213356990037955, mae 0.17012195380811235, rsquared -0.27690172807606883\n",
      "train epoch 52, loss 0.6816264629568356, nmse 0.3021033841896713, rmse 0.23083090807083087, mae 0.1859402928643011, rsquared -0.002710124880533238\n",
      "validation epoch 52, loss 0.0705148635743175, nmse 0.46919693932839807, rmse 0.2215300744727498, mae 0.1702212280532878, rsquared -0.2791454452564577\n",
      "train epoch 53, loss 0.6810675147795038, nmse 0.30188243833406303, rmse 0.23074648256685992, mae 0.18587783841085245, rsquared -0.001976784381673591\n",
      "validation epoch 53, loss 0.07046071437936972, nmse 0.4688340116416536, rmse 0.22144438023207302, mae 0.17015409425815645, rsquared -0.27815601574713233\n",
      "train epoch 54, loss 0.6804892891162204, nmse 0.30165584124700556, rmse 0.23065986557696802, mae 0.18581182104367627, rsquared -0.0012246869032874752\n",
      "validation epoch 54, loss 0.07044381560231673, nmse 0.4687146299802463, rmse 0.22141618466553042, mae 0.1701394382887968, rsquared -0.27783055218239805\n",
      "train epoch 55, loss 0.6799275290126081, nmse 0.30142958912733336, rmse 0.23057334798704787, mae 0.18574557324617236, rsquared -0.0004737344047593961\n",
      "validation epoch 55, loss 0.07051346132808138, nmse 0.46917483510418967, rmse 0.22152485618624967, mae 0.17018890743465834, rsquared -0.27908518374290736\n",
      "train epoch 56, loss 0.6793136833670133, nmse 0.30117990540194534, rmse 0.23047783258084822, mae 0.1856766303355552, rsquared 0.00035498984186232985\n",
      "validation epoch 56, loss 0.07043631347725884, nmse 0.4686608399749836, rmse 0.22140347936608437, mae 0.17010449777509926, rsquared -0.27768390749129956\n",
      "train epoch 57, loss 0.6786980402335373, nmse 0.30093034716096345, rmse 0.23038232560777644, mae 0.18560160713669008, rsquared 0.0011832975936973966\n",
      "validation epoch 57, loss 0.07042794911775498, nmse 0.4686021170823222, rmse 0.22138960807693608, mae 0.17008502737602296, rsquared -0.2775238145444281\n",
      "train epoch 58, loss 0.678111570498964, nmse 0.30070020586312574, rmse 0.2302942144736566, mae 0.18553518651644035, rsquared 0.0019471586477997072\n",
      "validation epoch 58, loss 0.07043545312963784, nmse 0.46864855251437204, rmse 0.22140057694269868, mae 0.17007868760926048, rsquared -0.2776504088727929\n",
      "train epoch 59, loss 0.6774711270990289, nmse 0.3004312082511621, rmse 0.23019118419119458, mae 0.1854568700073312, rsquared 0.0028399875375128003\n",
      "validation epoch 59, loss 0.07040712387351292, nmse 0.4684563769173863, rmse 0.2213551781566197, mae 0.17004780563001176, rsquared -0.2771264912617242\n",
      "train epoch 60, loss 0.6769159722381908, nmse 0.30021028851669146, rmse 0.23010653398498732, mae 0.18539429757355472, rsquared 0.003573241337809674\n",
      "validation epoch 60, loss 0.07038052520659113, nmse 0.46827112601177434, rmse 0.22131140641605393, mae 0.17002445821972303, rsquared -0.27662145205050837\n",
      "train epoch 61, loss 0.6763850403390068, nmse 0.30000551218658317, rmse 0.23002804165573912, mae 0.18532980027844054, rsquared 0.004252913629751109\n",
      "validation epoch 61, loss 0.0704650490666449, nmse 0.4688305791043735, rmse 0.2214435695853803, mae 0.17008388811451713, rsquared -0.27814665781220227\n",
      "train epoch 62, loss 0.6757931063024891, nmse 0.29977086342433396, rmse 0.2299380660528568, mae 0.1852644835730809, rsquared 0.00503173539081514\n",
      "validation epoch 62, loss 0.0703947704799454, nmse 0.4683587684294756, rmse 0.22133211595607377, mae 0.17001186809316524, rsquared -0.27686038668544954\n",
      "train epoch 63, loss 0.6751817460658139, nmse 0.2995224435367791, rmse 0.22984277155552274, mae 0.1851874669982606, rsquared 0.005856264838378533\n",
      "validation epoch 63, loss 0.07045753493972742, nmse 0.4687733049126453, rmse 0.22143004296254284, mae 0.17005347890847355, rsquared -0.2779905143778809\n",
      "train epoch 64, loss 0.67455169662631, nmse 0.2992620003462179, rmse 0.2297428224458543, mae 0.18511298008757826, rsquared 0.006720700782492095\n",
      "validation epoch 64, loss 0.07042575517055012, nmse 0.4685592771023927, rmse 0.2213794880382114, mae 0.1700130100639731, rsquared -0.27740702229663516\n",
      "train epoch 65, loss 0.6739286301246618, nmse 0.2990158619720337, rmse 0.22964832304885738, mae 0.1850388571031935, rsquared 0.007537657668222009\n",
      "validation epoch 65, loss 0.0703796914443151, nmse 0.46824778636343783, rmse 0.22130589102713064, mae 0.16996234340661198, rsquared -0.27655782246906613\n",
      "train epoch 66, loss 0.6733587451131727, nmse 0.2987892913060677, rmse 0.22956130185666906, mae 0.18497222740582642, rsquared 0.008289667452470173\n",
      "validation epoch 66, loss 0.0704563738815836, nmse 0.46875565809006237, rmse 0.22142587509140244, mae 0.17001562047648405, rsquared -0.2779424048297612\n",
      "train epoch 67, loss 0.6727152423692901, nmse 0.2985263294020261, rmse 0.22946026214991175, mae 0.18489573959179817, rsquared 0.009162463248345132\n",
      "validation epoch 67, loss 0.07036459448650925, nmse 0.4681419209650868, rmse 0.22128087226422466, mae 0.16992652101510197, rsquared -0.2762692075384041\n",
      "train epoch 68, loss 0.672118329637884, nmse 0.2982849064227427, rmse 0.22936745930591854, mae 0.18482077784102044, rsquared 0.009963769285864843\n",
      "validation epoch 68, loss 0.0704463598006733, nmse 0.46868539305957985, rmse 0.22140927894084858, mae 0.16999116167179618, rsquared -0.2777508451963351\n",
      "train epoch 69, loss 0.6714986309852234, nmse 0.29803823857715933, rmse 0.22927260154172185, mae 0.1847466333498051, rsquared 0.010782483537981835\n",
      "validation epoch 69, loss 0.07044606508436824, nmse 0.4686806490050842, rmse 0.22140815838070452, mae 0.16999091241600894, rsquared -0.27773791174517304\n",
      "train epoch 70, loss 0.6709046421343878, nmse 0.297800696790777, rmse 0.22918121615492323, mae 0.18467392105988678, rsquared 0.0115709075237862\n",
      "validation epoch 70, loss 0.0704586076568639, nmse 0.4687632178520585, rmse 0.22142766058469482, mae 0.1700106198790133, rsquared -0.27796301458722983\n",
      "train epoch 71, loss 0.6703015900398873, nmse 0.2975635782391873, rmse 0.229089957235186, mae 0.18460488951040355, rsquared 0.01235792675269598\n",
      "validation epoch 71, loss 0.07046364374134245, nmse 0.4687940604213662, rmse 0.22143494495063448, mae 0.17001466757625397, rsquared -0.278047098963625\n",
      "train epoch 72, loss 0.6696498961452565, nmse 0.2973004293514095, rmse 0.22898863754102963, mae 0.18452755970395773, rsquared 0.013231343165536313\n",
      "validation epoch 72, loss 0.07046898608799737, nmse 0.46882647127701277, rmse 0.22144259945460482, mae 0.17002341591252576, rsquared -0.2781354588715914\n",
      "train epoch 73, loss 0.6690173504694814, nmse 0.29704569368683675, rmse 0.22889051445171163, mae 0.184449632314862, rsquared 0.01407683528314485\n",
      "validation epoch 73, loss 0.07046503619991679, nmse 0.4687973530862338, rmse 0.22143572259464175, mae 0.17002103308138578, rsquared -0.2780560755721986\n",
      "train epoch 74, loss 0.6683743683189899, nmse 0.29678762421463684, rmse 0.2287910642815414, mae 0.18437138677829376, rsquared 0.014933392628212472\n",
      "validation epoch 74, loss 0.07048106539965676, nmse 0.4689015480976548, rmse 0.22146032940567706, mae 0.17003953490498447, rsquared -0.2783401366201421\n",
      "train epoch 75, loss 0.6677156239277515, nmse 0.2965193916484732, rmse 0.2286876518045519, mae 0.1842896390079843, rsquared 0.015823682257494087\n",
      "validation epoch 75, loss 0.0704696527404275, nmse 0.4688229450169561, rmse 0.22144176666709234, mae 0.17002579738901474, rsquared -0.2781258454254836\n",
      "train epoch 76, loss 0.6670858864055679, nmse 0.29627181049478674, rmse 0.2285921596078098, mae 0.1842146308639006, rsquared 0.01664542786685541\n",
      "validation epoch 76, loss 0.07049258695743066, nmse 0.4689727135091822, rmse 0.22147713433797128, mae 0.170053183013987, rsquared -0.2785341509121906\n",
      "train epoch 77, loss 0.6664234002520574, nmse 0.2960030634167464, rmse 0.22848845852889627, mae 0.18413465832883608, rsquared 0.017537425210433222\n",
      "validation epoch 77, loss 0.07050529288775752, nmse 0.4690563722456068, rmse 0.22149688780169519, mae 0.17006809230308695, rsquared -0.2787622250590651\n",
      "train epoch 78, loss 0.6657529367666916, nmse 0.2957312117716977, rmse 0.22838351158867795, mae 0.18405289980511133, rsquared 0.01843972690985629\n",
      "validation epoch 78, loss 0.07049821073585576, nmse 0.46900621220639827, rmse 0.22148504424697696, mae 0.17005806924020758, rsquared -0.27862547654195136\n",
      "train epoch 79, loss 0.6651134369625854, nmse 0.2954792515741233, rmse 0.2282862005604928, mae 0.1839745630181333, rsquared 0.01927600698613685\n",
      "validation epoch 79, loss 0.07052691179435171, nmse 0.469195333035357, rmse 0.2215296952689531, mae 0.17009109969597025, rsquared -0.2791410661093341\n",
      "train epoch 80, loss 0.66442097003007, nmse 0.2951967378585404, rmse 0.22817703992439237, mae 0.183889912286555, rsquared 0.020213697120897933\n",
      "validation epoch 80, loss 0.07051686102857978, nmse 0.46912608186086113, rmse 0.2215133462596836, mae 0.17007844590081464, rsquared -0.2789522704949341\n",
      "train epoch 81, loss 0.6637670477213756, nmse 0.29493776271214023, rmse 0.22807692847197295, mae 0.18381124705954982, rsquared 0.02107326048555347\n",
      "validation epoch 81, loss 0.07054170210760202, nmse 0.46928911625720254, rmse 0.22155183394876043, mae 0.17010758038025064, rsquared -0.27939674207609766\n",
      "train epoch 82, loss 0.6630693124004893, nmse 0.2946565350003249, rmse 0.22796816510202278, mae 0.1837256609344611, rsquared 0.022006682250392484\n",
      "validation epoch 82, loss 0.07054857257532111, nmse 0.4693332253126994, rmse 0.22156224566744342, mae 0.17011219432161667, rsquared -0.27951699413382425\n",
      "train epoch 83, loss 0.6623697334659434, nmse 0.29437287315451466, rmse 0.22785840775142913, mae 0.18363955065718687, rsquared 0.022948183139563816\n",
      "validation epoch 83, loss 0.07055601031571332, nmse 0.4693804989941765, rmse 0.22157340383687074, mae 0.17011964761151563, rsquared -0.2796458737348475\n",
      "train epoch 84, loss 0.6616844468997409, nmse 0.29409727276601827, rmse 0.2277517189621329, mae 0.1835541376620844, rsquared 0.02386292727825523\n",
      "validation epoch 84, loss 0.07056485475458646, nmse 0.46943725956396415, rmse 0.22158680048680535, mae 0.17012874409309897, rsquared -0.2798006169103209\n",
      "train epoch 85, loss 0.6609939741097838, nmse 0.2938257876730048, rmse 0.22764657437525174, mae 0.1834714907526361, rsquared 0.02476401235629533\n",
      "validation epoch 85, loss 0.07059200853624571, nmse 0.46961580953146886, rmse 0.2216289366337283, mae 0.1701569276075206, rsquared -0.28028738772773343\n",
      "train epoch 86, loss 0.6602650801974497, nmse 0.29352757820442943, rmse 0.2275310235959154, mae 0.1833804405099958, rsquared 0.025753798201554212\n",
      "validation epoch 86, loss 0.07059964641741497, nmse 0.4696651064712853, rmse 0.22164056884608183, mae 0.17016405442652432, rsquared -0.2804217832251157\n",
      "train epoch 87, loss 0.6596220049856034, nmse 0.29327663676510046, rmse 0.22743374283715467, mae 0.18332231783946823, rsquared 0.02658669692144844\n",
      "validation epoch 87, loss 0.07064892067258773, nmse 0.4699852207820705, rmse 0.22171608886183436, mae 0.1702012423062601, rsquared -0.2812944930155683\n",
      "train epoch 88, loss 0.6589214321806833, nmse 0.2929680047085306, rmse 0.22731404046089776, mae 0.1832096006939712, rsquared 0.027611076336513163\n",
      "validation epoch 88, loss 0.07090690031440976, nmse 0.47171405320401694, rmse 0.22212350391772645, mae 0.17049130018215342, rsquared -0.2860077124182989\n",
      "train epoch 89, loss 0.6581433773449954, nmse 0.2926805621714194, rmse 0.22720249968139786, mae 0.18311725143876328, rsquared 0.028565125702944005\n",
      "validation epoch 89, loss 0.07065115522973901, nmse 0.47000240452176034, rmse 0.22172014204964055, mae 0.17021569778726914, rsquared -0.2813413400866258\n",
      "train epoch 90, loss 0.6573695036103299, nmse 0.29236796983928176, rmse 0.22708113744908065, mae 0.18302299529578325, rsquared 0.029602649652684443\n",
      "validation epoch 90, loss 0.07067458449055272, nmse 0.47015687699207925, rmse 0.22175657467641502, mae 0.17023788561628678, rsquared -0.28176246976643315\n",
      "train epoch 91, loss 0.6566415212425334, nmse 0.292069203024183, rmse 0.22696508224406747, mae 0.1829303099195373, rsquared 0.030594285384542896\n",
      "validation epoch 91, loss 0.07070103245064502, nmse 0.47033466770324517, rmse 0.2217984995471046, mae 0.17026618563069593, rsquared -0.2822471706656382\n",
      "train epoch 92, loss 0.65575318751487, nmse 0.2917050820206169, rmse 0.22682356008716842, mae 0.18281807944289244, rsquared 0.031802837939942874\n",
      "validation epoch 92, loss 0.07115160294001645, nmse 0.4733530224508676, rmse 0.22250905307808352, mae 0.17075182937516462, rsquared -0.2904759428590713\n",
      "train epoch 93, loss 0.6551332790520403, nmse 0.2914661424325773, rmse 0.22673064392921574, mae 0.18273133486773607, rsquared 0.032595901363594026\n",
      "validation epoch 93, loss 0.07075370848817598, nmse 0.47068489960706833, rmse 0.2218810646441724, mae 0.1703183315669683, rsquared -0.2832019883701189\n",
      "train epoch 94, loss 0.6543403615935282, nmse 0.29114919482910884, rmse 0.2266073340992657, mae 0.18263887147270585, rsquared 0.03364788087685544\n",
      "validation epoch 94, loss 0.07078529707114734, nmse 0.4708945391214134, rmse 0.2219304712221869, mae 0.17034946144735327, rsquared -0.2837735168849982\n",
      "train epoch 95, loss 0.653536500785227, nmse 0.2908232817210334, rmse 0.22648046618819556, mae 0.18254540429798433, rsquared 0.03472961776032357\n",
      "validation epoch 95, loss 0.07080691704355495, nmse 0.4710376213425245, rmse 0.22196418566388393, mae 0.1703693287591777, rsquared -0.2841635939637053\n",
      "train epoch 96, loss 0.6527371445708465, nmse 0.2905003580884476, rmse 0.22635469185573684, mae 0.18244713405869506, rsquared 0.035801432287742574\n",
      "validation epoch 96, loss 0.07083027804996511, nmse 0.471192383467996, rmse 0.22200064647555895, mae 0.17039115489637366, rsquared -0.284585513314199\n",
      "train epoch 97, loss 0.6519360361982225, nmse 0.2901784400566907, rmse 0.22622923959187233, mae 0.18235155470284234, rsquared 0.03686990912950361\n",
      "validation epoch 97, loss 0.0708595710920926, nmse 0.4713869845797508, rmse 0.22204648456051032, mae 0.17041851342832556, rsquared -0.2851160434708946\n",
      "train epoch 98, loss 0.6511019964097129, nmse 0.28986287006013844, rmse 0.226106193612931, mae 0.182276901882324, rsquared 0.0379173162332016\n",
      "validation epoch 98, loss 0.07097786819866994, nmse 0.47218149791077274, rmse 0.22223353323051154, mae 0.1705388822991474, rsquared -0.28728208084962725\n",
      "train epoch 99, loss 0.6503048960978682, nmse 0.28951672880279233, rmse 0.2259711503403304, mae 0.18218632724788017, rsquared 0.039066192285387435\n",
      "validation epoch 99, loss 0.07128032898455398, nmse 0.47420180373080945, rmse 0.22270845703522082, mae 0.17086079343906913, rsquared -0.2927899279200368\n"
     ]
    }
   ],
   "source": [
    "from src.dl.loader import RegressDataset\n",
    "from src.dl.resnet import RegressCNN\n",
    "from sklearn.model_selection import KFold\n",
    "import collections\n",
    "train_set = RegressDataset('train')\n",
    "val_set = RegressDataset('validate')\n",
    "test_set = RegressDataset('test')\n",
    "fold_num = 5\n",
    "kf = KFold(n_splits=fold_num, shuffle=True, random_state=10)\n",
    "best_models = np.empty(dtype=collections.OrderedDict, shape=fold_num)\n",
    "net = RegressCNN().cuda()\n",
    "loss_fn = nn.SmoothL1Loss().cuda()\n",
    "#lr = 1e-2\n",
    "#optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "lr = 1e-2\n",
    "#optim = torch.optim.SGD([{'params': net.fc.weight, 'lr': 1e-2}], lr=lr)\n",
    "optim = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "epoch = 100\n",
    "train_loader = DataLoader(train_set, batch_size=8)\n",
    "val_loader = DataLoader(val_set, batch_size=8)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    predy = np.array([])\n",
    "    y = np.array([])\n",
    "    net.train()\n",
    "    for step, [img, labels, score] in enumerate(train_loader):\n",
    "        img = img.cuda()\n",
    "        labels = labels.cuda()\n",
    "        score = score.cuda()\n",
    "        output = net(img, labels).squeeze(-1)\n",
    "        loss = loss_fn(output, score)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        output = output.cpu().detach().numpy()\n",
    "        score = score.cpu().detach().numpy()\n",
    "        predy = np.concatenate((predy, output), axis=0)\n",
    "        y = np.concatenate((y, score), axis=0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(train_set)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(train_set)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = r2_score(y, predy)\n",
    "    print('train epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))\n",
    "    \n",
    "    if log:\n",
    "        writer.add_scalar('train/Loss', total_loss, i)\n",
    "        writer.add_scalar('train/NMSE', nmse, i)\n",
    "        writer.add_scalar('train/RMSE', rmse, i)\n",
    "        writer.add_scalar('train/MAE', mae, i)\n",
    "        writer.add_scalar('train/rsquared', rsquared, i)\n",
    "        \n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    predy = np.array([])\n",
    "    y = np.array([])\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for step, [img, labels, score] in enumerate(val_loader):\n",
    "            img = img.cuda()\n",
    "            labels = labels.cuda()\n",
    "            score = score.cuda()\n",
    "            output = net(img, labels).squeeze(-1)\n",
    "            loss = loss_fn(output, score)\n",
    "\n",
    "            output = output.cpu().detach().numpy()\n",
    "            score = score.cpu().detach().numpy()\n",
    "            #print(\"y: \", score)\n",
    "            #print(\"y_pred: \", output)\n",
    "            predy = np.concatenate((predy, output), axis=0)\n",
    "            y = np.concatenate((y, score), axis=0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(val_set)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(val_set)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = r2_score(y, predy)\n",
    "    print('validation epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))\n",
    "    if log:\n",
    "        writer.add_scalar('validation/Loss', total_loss, i)\n",
    "        writer.add_scalar('validation/NMSE', nmse, i)\n",
    "        writer.add_scalar('validation/RMSE', rmse, i)\n",
    "        writer.add_scalar('validation/MAE', mae, i)\n",
    "        writer.add_scalar('validation/rsquared', rsquared, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0, loss 76.2948952867059, nmse 0.2923725867191226, rmse 0.22708293039888186, mae 0.18030249206669766, rsquared 0.029587325785412788\n",
      "validation epoch 0, loss 7.459754966934131, nmse 0.42479690567412537, rmse 0.21078795018615362, mae 0.16226482330006475, rsquared -0.15810011000898005\n",
      "train epoch 1, loss 66.04962458012086, nmse 0.2531131950811844, rmse 0.21128738004466602, mae 0.16673977112354485, rsquared 0.15989301434167025\n",
      "validation epoch 1, loss 7.446343205243306, nmse 0.4239673417145707, rmse 0.21058203114873628, mae 0.16533574936384027, rsquared -0.1558385160567004\n",
      "train epoch 2, loss 56.31746606768795, nmse 0.21581676944158154, rmse 0.19510066898222367, mae 0.15286047245792922, rsquared 0.2836834303642999\n",
      "validation epoch 2, loss 7.671067939525478, nmse 0.4366770406693776, rmse 0.2137151390018426, mae 0.1714066501234379, rsquared -0.19048825940731318\n",
      "train epoch 3, loss 46.38304410493647, nmse 0.17774252926918788, rmse 0.17705648690164505, mae 0.13793395655235102, rsquared 0.4100554874678478\n",
      "validation epoch 3, loss 7.931797393339527, nmse 0.45143390778921266, rmse 0.2172962324563272, mae 0.17519487267806072, rsquared -0.2307190831411825\n",
      "train epoch 4, loss 37.55761350685016, nmse 0.14392268804341296, rmse 0.15932385122601386, mae 0.12359737214427016, rsquared 0.5223067861744048\n",
      "validation epoch 4, loss 8.17136956509812, nmse 0.46506423045351164, rmse 0.22055229292741585, mae 0.1773833312187929, rsquared -0.26787867156128264\n",
      "train epoch 5, loss 30.43784342391877, nmse 0.11663873831512879, rmse 0.14342919849391889, mae 0.11064584687478705, rsquared 0.6128648337535929\n",
      "validation epoch 5, loss 8.428838300098064, nmse 0.4796946433914008, rmse 0.22399459695062066, mae 0.17951806775617413, rsquared -0.30776475031215744\n",
      "train epoch 6, loss 24.91598963485642, nmse 0.09547894226014338, rmse 0.12976870313739788, mae 0.09932974122435262, rsquared 0.6830962275582388\n",
      "validation epoch 6, loss 8.712184949533551, nmse 0.49581839853878185, rmse 0.22772799758586482, mae 0.18219354126830403, rsquared -0.3517220446345046\n",
      "train epoch 7, loss 20.69441416098366, nmse 0.07930220451260957, rmse 0.11826569684846264, mae 0.08972036732825023, rsquared 0.7367883726181081\n",
      "validation epoch 7, loss 8.943803514772075, nmse 0.509058876613216, rmse 0.23074862173340277, mae 0.18425998326716508, rsquared -0.38781882149364844\n",
      "train epoch 8, loss 17.44500727323506, nmse 0.0668502166586357, rmse 0.10858443463782733, mae 0.08156614400841232, rsquared 0.778117715318321\n",
      "validation epoch 8, loss 9.162711020340613, nmse 0.5215903279316435, rmse 0.2335715124893642, mae 0.18622830421770245, rsquared -0.42198261825533256\n",
      "train epoch 9, loss 14.938743337468443, nmse 0.057247035571613265, rmse 0.10048301697088119, mae 0.07467950642353942, rsquared 0.8099915949600436\n",
      "validation epoch 9, loss 9.328482662543967, nmse 0.5310566953652954, rmse 0.23568153201890746, mae 0.18788899493592143, rsquared -0.4477902477833777\n",
      "train epoch 10, loss 12.968291411263941, nmse 0.049696013983477644, rmse 0.09362179057582015, mae 0.0687326035098018, rsquared 0.8350541602799388\n",
      "validation epoch 10, loss 9.497386483402046, nmse 0.5407011303433666, rmse 0.2378119899572914, mae 0.18932033033091586, rsquared -0.4740833329256855\n",
      "train epoch 11, loss 11.37533351583898, nmse 0.043592847676232956, rmse 0.08768469282059295, mae 0.06351936077926153, rsquared 0.8553111549723975\n",
      "validation epoch 11, loss 9.58844845734747, nmse 0.5459055257928319, rmse 0.23895375163912105, mae 0.19013473797628602, rsquared -0.4882717859535888\n",
      "train epoch 12, loss 10.083647912152768, nmse 0.038642838026533954, rmse 0.08255638585400837, mae 0.05896622143864583, rsquared 0.8717407120504252\n",
      "validation epoch 12, loss 9.714797772687692, nmse 0.5531283156891076, rmse 0.24052933688119987, mae 0.19120706613368926, rsquared -0.5079628751816856\n",
      "train epoch 13, loss 9.013466078852302, nmse 0.034541958275314213, rmse 0.07805300763986871, mae 0.054849233331714375, rsquared 0.885351925504704\n",
      "validation epoch 13, loss 9.853551832519068, nmse 0.561068562443864, rmse 0.24224960403772233, mae 0.1921299495017128, rsquared -0.5296099270254111\n",
      "train epoch 14, loss 8.117402165611864, nmse 0.031108803583007924, rmse 0.0740726392911417, mae 0.05117147580371325, rsquared 0.896746895407111\n",
      "validation epoch 14, loss 9.912221128345696, nmse 0.5644151718185246, rmse 0.24297100381230013, mae 0.19291355136087412, rsquared -0.5387336014994539\n",
      "train epoch 15, loss 7.363358094860422, nmse 0.028219063031068952, rmse 0.07054844728213652, mae 0.04781444082924412, rsquared 0.906338221626376\n",
      "validation epoch 15, loss 10.043599766891532, nmse 0.5719363553278043, rmse 0.24458451627351158, mae 0.19378671174537757, rsquared -0.5592381845911596\n",
      "train epoch 16, loss 6.724610569349889, nmse 0.02577116111051505, rmse 0.06741913216649645, mae 0.044731730825746906, rsquared 0.9144630430249766\n",
      "validation epoch 16, loss 10.116932095825977, nmse 0.5761188675545762, rmse 0.24547719803315893, mae 0.19461294245338553, rsquared -0.5706407343866948\n",
      "train epoch 17, loss 6.180446868068347, nmse 0.023686334504343525, rmse 0.06463460424765151, mae 0.04190343900388701, rsquared 0.9213827826109325\n",
      "validation epoch 17, loss 10.209268469985314, nmse 0.5814071895276017, rmse 0.2466012690348559, mae 0.19516206804836264, rsquared -0.5850579916146039\n",
      "train epoch 18, loss 5.7236595413286135, nmse 0.021935839902349607, rmse 0.06220041736123849, mae 0.03934963885172176, rsquared 0.9271928421892986\n",
      "validation epoch 18, loss 10.292410506202833, nmse 0.5861334745000122, rmse 0.24760155666051487, mae 0.19592498323687735, rsquared -0.5979429987165181\n",
      "train epoch 19, loss 5.310593273673522, nmse 0.02035272419150752, rmse 0.05991387907842181, mae 0.03686182987449429, rsquared 0.9324473551646388\n",
      "validation epoch 19, loss 10.406618057868677, nmse 0.5926676935971585, rmse 0.24897786325984675, mae 0.19669334368826025, rsquared -0.6157568757814162\n",
      "train epoch 20, loss 4.962130351806734, nmse 0.01901755307551221, rmse 0.057915323080288524, mae 0.03456043092168541, rsquared 0.9368789162345273\n",
      "validation epoch 20, loss 10.504815865370073, nmse 0.5982669002710028, rmse 0.250151203197431, mae 0.19751024948919696, rsquared -0.6310216806289275\n",
      "train epoch 21, loss 4.669243048903032, nmse 0.017895173974524628, rmse 0.056180309491034704, mae 0.03250903732381526, rsquared 0.9406042001850308\n",
      "validation epoch 21, loss 10.558118005743404, nmse 0.6012937985424328, rmse 0.2507832178793017, mae 0.19809294021985005, rsquared -0.6392737445547843\n",
      "train epoch 22, loss 4.41897727823817, nmse 0.01693596131858567, rmse 0.05465389164575827, mae 0.03057885033889562, rsquared 0.9437879190453918\n",
      "validation epoch 22, loss 10.595915877878635, nmse 0.6034550440139121, rmse 0.25123351184029796, mae 0.19836007482505588, rsquared -0.6451658275357199\n",
      "train epoch 23, loss 4.225689578084899, nmse 0.016195189345430492, rmse 0.05344525823954847, mae 0.02895205759676451, rsquared 0.9462466123159183\n",
      "validation epoch 23, loss 10.70721967158416, nmse 0.6098137125178029, rmse 0.25255368007605516, mae 0.19923563288795046, rsquared -0.6625011108099244\n",
      "train epoch 24, loss 4.03149601347693, nmse 0.015450981985704904, rmse 0.05220284929775811, mae 0.027233874655587128, rsquared 0.9487167079641649\n",
      "validation epoch 24, loss 10.782728599640775, nmse 0.6141011849639432, rmse 0.25343995101340316, mae 0.19981582894276542, rsquared -0.6741898077971491\n",
      "train epoch 25, loss 3.8720678889261695, nmse 0.014839993051224484, rmse 0.051160291896683086, mae 0.02569469400690774, rsquared 0.9507446388740974\n",
      "validation epoch 25, loss 10.850830614160028, nmse 0.6179790049145254, rmse 0.25423888114614773, mae 0.20031053758624423, rsquared -0.6847616920349533\n",
      "train epoch 26, loss 3.7531873266974465, nmse 0.014384360399556012, rmse 0.050368781275113926, mae 0.024469981758037242, rsquared 0.9522569273718897\n",
      "validation epoch 26, loss 10.92900270263231, nmse 0.6224320120566766, rmse 0.2551532288831109, mae 0.20096079223940075, rsquared -0.6969016770308709\n",
      "train epoch 27, loss 3.635180208740391, nmse 0.013932083895288307, rmse 0.049570603399823956, mae 0.02313627290043996, rsquared 0.9537580764943705\n",
      "validation epoch 27, loss 10.94041728583926, nmse 0.6230732083185929, rmse 0.25528461767839133, mae 0.20118724436018814, rsquared -0.698649734635679\n",
      "train epoch 28, loss 3.5448518212836286, nmse 0.01358584946684182, rmse 0.04895077409552894, mae 0.02205274700098704, rsquared 0.9549072617903801\n",
      "validation epoch 28, loss 11.02807574021995, nmse 0.6280632857209519, rmse 0.25630484255776187, mae 0.2019272082171543, rsquared -0.7122539043257912\n",
      "train epoch 29, loss 3.4728636097329515, nmse 0.013309945120026923, rmse 0.04845117317127333, mae 0.02117824849674633, rsquared 0.9558230147959014\n",
      "validation epoch 29, loss 11.066709456783391, nmse 0.6302669349043426, rmse 0.25675408980051456, mae 0.20230786934239664, rsquared -0.7182615901813554\n",
      "train epoch 30, loss 3.4034309397790605, nmse 0.013043769409506677, rmse 0.0479642569885421, mae 0.020216502275675886, rsquared 0.9567064775239069\n",
      "validation epoch 30, loss 11.102377231723187, nmse 0.6322819043895219, rmse 0.25716418494640303, mae 0.2026806870904462, rsquared -0.7237548890996883\n",
      "train epoch 31, loss 3.339034362603297, nmse 0.012796897940526512, rmse 0.047508193651534206, mae 0.019260918713385887, rsquared 0.9575258676216197\n",
      "validation epoch 31, loss 11.092583845800917, nmse 0.6317088630410803, rmse 0.25704762371393486, mae 0.20277025533337692, rsquared -0.7221926384340045\n",
      "train epoch 32, loss 3.2848764841667073, nmse 0.012589235213020114, rmse 0.04712114542948583, mae 0.01843008436475961, rsquared 0.958215120143532\n",
      "validation epoch 32, loss 11.122203457385462, nmse 0.6334022754985312, rmse 0.2573919249788751, mae 0.2030506157539658, rsquared -0.726809294363153\n",
      "train epoch 33, loss 3.2441363594453105, nmse 0.01243320446185067, rmse 0.04682822568152331, mae 0.017753458414990608, rsquared 0.9587330011808795\n",
      "validation epoch 33, loss 11.177817685813045, nmse 0.6365657385911, rmse 0.25803388329305843, mae 0.20339539548701865, rsquared -0.7354336673436939\n",
      "train epoch 34, loss 3.2090680632015243, nmse 0.012298835156094331, rmse 0.046574495064228956, mae 0.01715887356227143, rsquared 0.9591789858020595\n",
      "validation epoch 34, loss 11.210220941437194, nmse 0.6384061003009452, rmse 0.2584066122347744, mae 0.20369617003234072, rsquared -0.7404509428232453\n",
      "train epoch 35, loss 3.1809416260005405, nmse 0.01219103327107494, rmse 0.04636992817172479, mae 0.0166279902489087, rsquared 0.9595367906041476\n",
      "validation epoch 35, loss 11.230228253026382, nmse 0.639545209370667, rmse 0.25863704716386443, mae 0.2038085823955074, rsquared -0.7435564323438513\n",
      "train epoch 36, loss 3.15159142861044, nmse 0.012078555043415276, rmse 0.04615552085938306, mae 0.01605673818302181, rsquared 0.9599101166362459\n",
      "validation epoch 36, loss 11.218390366101474, nmse 0.6388437735591057, rmse 0.258495175209307, mae 0.20380848703560617, rsquared -0.7416441470148325\n",
      "train epoch 37, loss 3.1363519826629043, nmse 0.01202013606076837, rmse 0.04604376797321731, mae 0.0157176825775657, rsquared 0.9601040148461001\n",
      "validation epoch 37, loss 11.233379148452727, nmse 0.6396996812705095, rmse 0.2586682800957199, mae 0.2039984500843234, rsquared -0.7439775604684009\n",
      "train epoch 38, loss 3.11747074402817, nmse 0.011947758786605646, rmse 0.04590493617794819, mae 0.015329005398915008, rsquared 0.9603442419650676\n",
      "validation epoch 38, loss 11.27086670062091, nmse 0.6418328649203846, rmse 0.25909920707702294, mae 0.20430901791189746, rsquared -0.7497931400703033\n",
      "train epoch 39, loss 3.106306051397087, nmse 0.01190496512013787, rmse 0.04582265284995675, mae 0.015080158497361334, rsquared 0.9604862782509672\n",
      "validation epoch 39, loss 11.266414311724322, nmse 0.6415752920791429, rmse 0.25904721253183716, mae 0.20434298077443153, rsquared -0.7490909335998865\n",
      "train epoch 40, loss 3.082349542352992, nmse 0.011813155366254643, rmse 0.04564562130144191, mae 0.01450866652132549, rsquared 0.9607910036350552\n",
      "validation epoch 40, loss 11.288401771172854, nmse 0.6428257857661455, rmse 0.25929954395363697, mae 0.20450744653430486, rsquared -0.7525000847898771\n",
      "train epoch 41, loss 3.0658056758167973, nmse 0.011749747771210533, rmse 0.04552295410415995, mae 0.014100897722110272, rsquared 0.9610014595282111\n",
      "validation epoch 41, loss 11.298282059138105, nmse 0.6433823317004806, rmse 0.25941176789364123, mae 0.20462168012235238, rsquared -0.7540173649281487\n",
      "train epoch 42, loss 3.0516398185437406, nmse 0.011695444958458864, rmse 0.04541763749198775, mae 0.013728626796015359, rsquared 0.9611816957751557\n",
      "validation epoch 42, loss 11.30295934181832, nmse 0.6436364620081326, rmse 0.25946299550787105, mae 0.20474950526952046, rsquared -0.7547101862112542\n",
      "train epoch 43, loss 3.0407675431395247, nmse 0.011653778841876952, rmse 0.04533666301703367, mae 0.013427763754964723, rsquared 0.9613199896147735\n",
      "validation epoch 43, loss 11.298155641773395, nmse 0.6433596174847988, rmse 0.25940718866713525, mae 0.204742492403174, rsquared -0.7539554404288678\n",
      "train epoch 44, loss 3.0367448275561264, nmse 0.011638357798327292, rmse 0.04530665686607007, mae 0.013312071973105667, rsquared 0.961371173538267\n",
      "validation epoch 44, loss 11.30129586714819, nmse 0.6435380693749749, rmse 0.25944316271049256, mae 0.2048445480740692, rsquared -0.7544419438635301\n",
      "train epoch 45, loss 3.0310752226151747, nmse 0.011616625169597019, rmse 0.04526433591076698, mae 0.013131784778054007, rsquared 0.9614433062186954\n",
      "validation epoch 45, loss 11.303417646401286, nmse 0.6436418356519173, rmse 0.25946407861835763, mae 0.20484027725942847, rsquared -0.7547248360765784\n",
      "train epoch 46, loss 3.0226677875540777, nmse 0.011584401453072097, rmse 0.04520151224138163, mae 0.012898235389431459, rsquared 0.9615502598263403\n",
      "validation epoch 46, loss 11.318587065257958, nmse 0.6445105040834327, rmse 0.2596391078397201, mae 0.20497378721073906, rsquared -0.7570930383696313\n",
      "train epoch 47, loss 3.0157680350520177, nmse 0.011557946209078806, rmse 0.04514986949828466, mae 0.012675893505363722, rsquared 0.9616380673200545\n",
      "validation epoch 47, loss 11.291420395157878, nmse 0.6429593253636905, rmse 0.25932647579066803, mae 0.20482558310995244, rsquared -0.7528641463462156\n",
      "train epoch 48, loss 3.017440530021287, nmse 0.011564361636641643, rmse 0.045162398347641255, mae 0.012714155743412368, rsquared 0.9616167739002871\n",
      "validation epoch 48, loss 11.317901429783094, nmse 0.6444661531751054, rmse 0.2596301743713715, mae 0.20496566386128615, rsquared -0.7569721269620227\n",
      "train epoch 49, loss 3.0043954564911775, nmse 0.011514368956924155, rmse 0.0450646742026109, mae 0.012276397762307567, rsquared 0.9617827044020493\n",
      "validation epoch 49, loss 11.316999891857627, nmse 0.6444018235877473, rmse 0.25961721611226757, mae 0.2050590383171572, rsquared -0.7567967487961249\n",
      "train epoch 50, loss 2.9943124330963795, nmse 0.011475718197002388, rmse 0.04498897539323415, mae 0.011936859413661939, rsquared 0.9619109899835294\n",
      "validation epoch 50, loss 11.318823895000342, nmse 0.6445005568394256, rmse 0.25963710422340325, mae 0.2050782410442257, rsquared -0.7570659197531231\n",
      "train epoch 51, loss 2.9885179705884193, nmse 0.011453511519719187, rmse 0.04494542520197905, mae 0.011730271451492446, rsquared 0.9619846960765993\n",
      "validation epoch 51, loss 11.319827312109451, nmse 0.6445598595639358, rmse 0.25964904900244967, mae 0.20509945900625118, rsquared -0.7572275934632222\n",
      "train epoch 52, loss 2.9887369837045217, nmse 0.01145434489051822, rmse 0.044947060312902645, mae 0.011745847493581523, rsquared 0.961981930038939\n",
      "validation epoch 52, loss 11.326485490884306, nmse 0.6449335564264939, rmse 0.25972430653968515, mae 0.20509750015936334, rsquared -0.7582463823758954\n",
      "train epoch 53, loss 2.9823154837011137, nmse 0.01142973584180335, rmse 0.04489875117134845, mae 0.01148081999539124, rsquared 0.9620636098333454\n",
      "validation epoch 53, loss 11.333954831008539, nmse 0.6453528603707275, rmse 0.259808722779083, mae 0.20518385607828962, rsquared -0.7593895073315104\n",
      "train epoch 54, loss 2.979215517003189, nmse 0.011417855171892763, rmse 0.04487541003995058, mae 0.011363508309926804, rsquared 0.9621030429169622\n",
      "validation epoch 54, loss 11.35122596752313, nmse 0.6463344572102526, rmse 0.2600062352833455, mae 0.20526252668148423, rsquared -0.7620655800445038\n",
      "train epoch 55, loss 2.9836718601238865, nmse 0.01143493749668389, rmse 0.044908966684883804, mae 0.011534280843619276, rsquared 0.9620463450415958\n",
      "validation epoch 55, loss 11.35716446701661, nmse 0.6466648000274615, rmse 0.26007267164559694, mae 0.20531483109620943, rsquared -0.7629661752415029\n",
      "train epoch 56, loss 2.9800105453766936, nmse 0.011420898267583874, rmse 0.04488138975623947, mae 0.011390862512715297, rsquared 0.9620929425903186\n",
      "validation epoch 56, loss 11.349041956115332, nmse 0.6462075119527468, rmse 0.25998070037725307, mae 0.2052592928436414, rsquared -0.7617194962696012\n",
      "train epoch 57, loss 2.971159660849624, nmse 0.01138697983583279, rmse 0.044814694551577335, mae 0.01102797250917183, rsquared 0.9622055211204404\n",
      "validation epoch 57, loss 11.352812113159862, nmse 0.6464171979990289, rmse 0.2600228771568332, mae 0.2053099199670531, rsquared -0.7622911516419049\n",
      "train epoch 58, loss 2.9664016350340443, nmse 0.011368744388313645, rmse 0.044778796382655375, mae 0.010827601435911851, rsquared 0.9622660463208059\n",
      "validation epoch 58, loss 11.36661193551456, nmse 0.6472139465281287, rmse 0.2601830748175512, mae 0.20540996073051496, rsquared -0.7644632827164843\n",
      "train epoch 59, loss 2.9639999097324776, nmse 0.01135953762673534, rmse 0.0447606610816679, mae 0.010719829505606817, rsquared 0.9622966044460539\n",
      "validation epoch 59, loss 11.3742980296265, nmse 0.6476505293717023, rmse 0.26027081421017007, mae 0.20547268885531733, rsquared -0.7656535141716034\n",
      "train epoch 60, loss 2.9618787662258423, nmse 0.01135140916897608, rmse 0.04474464369324864, mae 0.010623147263176725, rsquared 0.9623235835774422\n",
      "validation epoch 60, loss 11.37450311680634, nmse 0.6476626608016843, rmse 0.260273251822918, mae 0.2055053841797448, rsquared -0.7656865874125078\n",
      "train epoch 61, loss 2.96151584154417, nmse 0.011350013816381755, rmse 0.0447418935292596, mae 0.010609375256261424, rsquared 0.9623282148866142\n",
      "validation epoch 61, loss 11.369608943756814, nmse 0.6473759393869826, rmse 0.26021563372898276, mae 0.20551188492319386, rsquared -0.7649049148120894\n",
      "train epoch 62, loss 2.959895695467457, nmse 0.01134379837040673, rmse 0.04472964116953982, mae 0.0105703128294026, rsquared 0.9623488445482995\n",
      "validation epoch 62, loss 11.397059950184223, nmse 0.6489489097661245, rmse 0.2605315731455107, mae 0.20560976630622962, rsquared -0.7691932162210515\n",
      "train epoch 63, loss 2.9575361150344333, nmse 0.011334761554873795, rmse 0.04471182112209252, mae 0.010462734417173143, rsquared 0.9623788386063133\n",
      "validation epoch 63, loss 11.391374729776821, nmse 0.6486244351398108, rmse 0.2604664320741561, mae 0.20565266147395814, rsquared -0.7683086191455792\n",
      "train epoch 64, loss 2.9546111444647316, nmse 0.011323551728698974, rmse 0.04468970615639899, mae 0.010297714936998041, rsquared 0.9624160450951906\n",
      "validation epoch 64, loss 11.401778200903685, nmse 0.6492185675366661, rmse 0.2605856968719159, mae 0.2057160882724884, rsquared -0.7699283691601564\n",
      "train epoch 65, loss 2.9527883542144924, nmse 0.011316566325440328, rmse 0.044675919679065125, mae 0.010218128214446271, rsquared 0.9624392303189927\n",
      "validation epoch 65, loss 11.410928347552092, nmse 0.6497404090601376, rmse 0.2606904051541278, mae 0.20580069125187472, rsquared -0.771351036598801\n",
      "train epoch 66, loss 2.951340297049235, nmse 0.011311016378820067, rmse 0.044664963202959394, mae 0.010137350172884436, rsquared 0.9624576511244518\n",
      "validation epoch 66, loss 11.41285048440373, nmse 0.6498470432108894, rmse 0.26071179628134533, mae 0.20578708938395426, rsquared -0.7716417473362542\n",
      "train epoch 67, loss 2.950810827856242, nmse 0.011308986836796852, rmse 0.04466095589347306, mae 0.010107023401706156, rsquared 0.9624643873692013\n",
      "validation epoch 67, loss 11.413231182640146, nmse 0.6498794587285415, rmse 0.2607182985825687, mae 0.20581462814751028, rsquared -0.7717301199539863\n",
      "train epoch 68, loss 2.9486390916360015, nmse 0.011300661655785168, rmse 0.04464451414673687, mae 0.00998837014694211, rsquared 0.9624920194439437\n",
      "validation epoch 68, loss 11.433298115951636, nmse 0.6510176097417265, rmse 0.2609465001241044, mae 0.2059716439375354, rsquared -0.774832997578494\n",
      "train epoch 69, loss 2.947786050425054, nmse 0.011297392800181416, rmse 0.04463805669186838, mae 0.00994592053628269, rsquared 0.9625028690894035\n",
      "validation epoch 69, loss 11.426183277528414, nmse 0.6506144628368131, rmse 0.2608656911999256, mae 0.2059466018046803, rsquared -0.7737339206579854\n",
      "train epoch 70, loss 2.9465514628347513, nmse 0.011292658612796205, rmse 0.04462870289350684, mae 0.009868942638738063, rsquared 0.9625185823116733\n",
      "validation epoch 70, loss 11.448139803746667, nmse 0.6518658879383094, rmse 0.2611164518761781, mae 0.20610524936662933, rsquared -0.7771456111113582\n",
      "train epoch 71, loss 2.94524145785102, nmse 0.011287639920927494, rmse 0.044618784830514756, mae 0.009781282113852393, rsquared 0.9625352398316276\n",
      "validation epoch 71, loss 11.450787963395582, nmse 0.6520228770363725, rmse 0.2611478923682097, mae 0.2061812493079131, rsquared -0.7775736017330146\n",
      "train epoch 72, loss 2.9442859085510755, nmse 0.011283976405626927, rmse 0.044611543508702244, mae 0.00972609737199106, rsquared 0.9625473993904965\n",
      "validation epoch 72, loss 11.458252320668054, nmse 0.6524487626831467, rmse 0.2612331662040475, mae 0.20623102458095813, rsquared -0.7787346700171578\n",
      "train epoch 73, loss 2.9447328332721785, nmse 0.011285688474390954, rmse 0.04461492773827273, mae 0.00975427516187869, rsquared 0.962541716869962\n",
      "validation epoch 73, loss 11.456599376918799, nmse 0.6523582405897894, rmse 0.2612150435600255, mae 0.20623879461849923, rsquared -0.778487884683096\n",
      "train epoch 74, loss 2.9432376415578774, nmse 0.011279958791606662, rmse 0.044603600922118986, mae 0.009654120028450678, rsquared 0.9625607342369987\n",
      "validation epoch 74, loss 11.467643516096963, nmse 0.652980865525655, rmse 0.2613396684988875, mae 0.20637104673686238, rsquared -0.7801853123175464\n",
      "train epoch 75, loss 2.941905010904261, nmse 0.011274850968362893, rmse 0.04459350101346905, mae 0.009572297268300738, rsquared 0.9625776875925408\n",
      "validation epoch 75, loss 11.467821597874776, nmse 0.6529911890139717, rmse 0.2613417343529527, mae 0.20639540927684058, rsquared -0.78021345666793\n",
      "train epoch 76, loss 2.9409790622979557, nmse 0.011271298204337065, rmse 0.04458647463816968, mae 0.009506189658267994, rsquared 0.9625894795573\n",
      "validation epoch 76, loss 11.474487370012195, nmse 0.6533793378920848, rmse 0.2614193957615937, mae 0.20646708616691634, rsquared -0.7812716453045192\n",
      "train epoch 77, loss 2.94627621721352, nmse 0.011291605475967345, rmse 0.044626621841201315, mae 0.009814948813050265, rsquared 0.9625220777738774\n",
      "validation epoch 77, loss 11.479613500043918, nmse 0.6536766022351844, rmse 0.2614788572593499, mae 0.20653939289603776, rsquared -0.7820820605025751\n",
      "train epoch 78, loss 2.944527194246492, nmse 0.011284899574446906, rmse 0.04461336835936625, mae 0.009716706166652848, rsquared 0.9625443353045868\n",
      "validation epoch 78, loss 11.480451398255916, nmse 0.6537288872009833, rmse 0.2614893143720053, mae 0.20652465751768637, rsquared -0.78222460208853\n",
      "train epoch 79, loss 2.941186818213426, nmse 0.011272097684801752, rmse 0.044588055883858495, mae 0.009501689838767034, rsquared 0.9625868260049119\n",
      "validation epoch 79, loss 11.479420422474744, nmse 0.6536752644323838, rmse 0.2614785896902452, mae 0.20662960901237656, rsquared -0.7820784133254186\n",
      "train epoch 80, loss 2.940193279286192, nmse 0.011268290320206471, rmse 0.044580525017480305, mae 0.009449231166830094, rsquared 0.962599463013395\n",
      "validation epoch 80, loss 11.488118429816582, nmse 0.6541726433960807, rmse 0.26157804979457044, mae 0.2066714434752658, rsquared -0.7834343898517999\n",
      "train epoch 81, loss 2.940125684699492, nmse 0.011268028653055474, rmse 0.04458000740006178, mae 0.009451951335305277, rsquared 0.9626003315117813\n",
      "validation epoch 81, loss 11.48646744150261, nmse 0.6540741169173988, rmse 0.26155835061050253, mae 0.20675967467098227, rsquared -0.7831657826084886\n",
      "train epoch 82, loss 2.938449642978592, nmse 0.011261605338015818, rmse 0.044567299220975805, mae 0.009328973945236664, rsquared 0.9626216511108412\n",
      "validation epoch 82, loss 11.494837448071804, nmse 0.654560379863303, rmse 0.2616555586339251, mae 0.20680775442147223, rsquared -0.7844914541555807\n",
      "train epoch 83, loss 2.9371603064779763, nmse 0.011256664953153618, rmse 0.04455752247259446, mae 0.009237993364029787, rsquared 0.9626380487223258\n",
      "validation epoch 83, loss 11.498411892711331, nmse 0.6547665309583217, rmse 0.2616967590603237, mae 0.20692144130476126, rsquared -0.7850534723874245\n",
      "train epoch 84, loss 2.942432148210595, nmse 0.011276868254218987, rmse 0.04459749014892136, mae 0.009544797993224604, rsquared 0.9625709920271862\n",
      "validation epoch 84, loss 11.499011241312054, nmse 0.6548082533623849, rmse 0.2617050967222813, mae 0.20691648161182985, rsquared -0.7851672178504681\n",
      "train epoch 85, loss 2.939602134494169, nmse 0.011266025338565478, rmse 0.04457604434018047, mae 0.009398881205867541, rsquared 0.962606980704831\n",
      "validation epoch 85, loss 11.493237787414214, nmse 0.6544809917745467, rmse 0.2616396907589126, mae 0.20689063498750068, rsquared -0.7842750228372397\n",
      "train epoch 86, loss 2.937382729538785, nmse 0.011257516868549726, rmse 0.044559208519100856, mae 0.009256006603833623, rsquared 0.9626352211333682\n",
      "validation epoch 86, loss 11.493472936298694, nmse 0.654499260175151, rmse 0.26164334228232683, mae 0.2069748587209315, rsquared -0.7843248269588496\n",
      "train epoch 87, loss 2.9373209033449834, nmse 0.011257280963852133, rmse 0.04455874164075623, mae 0.009248060424728461, rsquared 0.9626360041236991\n",
      "validation epoch 87, loss 11.498110929591952, nmse 0.654763976264245, rmse 0.26169624853040135, mae 0.2070274209741836, rsquared -0.7850465076674564\n",
      "train epoch 88, loss 2.9359402520072995, nmse 0.01125198923198994, rmse 0.04454826750272684, mae 0.00913527185455032, rsquared 0.9626535678895953\n",
      "validation epoch 88, loss 11.504038471049974, nmse 0.6551124578538915, rmse 0.26176587987619215, mae 0.20708422800102116, rsquared -0.7859965535880244\n",
      "train epoch 89, loss 2.9352369641284763, nmse 0.01124929332900853, rmse 0.04454293044635285, mae 0.009082444874790778, rsquared 0.9626625158503156\n",
      "validation epoch 89, loss 11.513464830141446, nmse 0.6556455522232963, rmse 0.26187236353242904, mae 0.20720112099930604, rsquared -0.7874498990328864\n",
      "train epoch 90, loss 2.9356946224490756, nmse 0.011251045159830563, rmse 0.04454639860379542, mae 0.009109490452539325, rsquared 0.9626567013556946\n",
      "validation epoch 90, loss 11.516736474966734, nmse 0.6558463396842465, rmse 0.2619124588718818, mae 0.20724490123104494, rsquared -0.7879972946883371\n",
      "train epoch 91, loss 2.935568629701727, nmse 0.011250564498788203, rmse 0.04454544705010543, mae 0.009112826385383015, rsquared 0.962658296715823\n",
      "validation epoch 91, loss 11.511067672194125, nmse 0.6555182805716888, rmse 0.26184694541917775, mae 0.20723670268213337, rsquared -0.7871029254279496\n",
      "train epoch 92, loss 2.93372105678572, nmse 0.011243483543314203, rmse 0.0445314266881157, mae 0.008996565579642972, rsquared 0.9626817990865981\n",
      "validation epoch 92, loss 11.508941902183754, nmse 0.6553991815017217, rmse 0.2618231572615965, mae 0.20727445442304598, rsquared -0.7867782322764967\n",
      "train epoch 93, loss 2.9329254456599303, nmse 0.011240433832708138, rmse 0.04452538687073068, mae 0.008925696833377121, rsquared 0.9626919213687792\n",
      "validation epoch 93, loss 11.507869928382696, nmse 0.655341137097676, rmse 0.26181156302555286, mae 0.2072398785154767, rsquared -0.7866199890552941\n",
      "train epoch 94, loss 2.9338518854695055, nmse 0.011243983384305678, rmse 0.044532416523027835, mae 0.009000340975072931, rsquared 0.9626801400663778\n",
      "validation epoch 94, loss 11.514380498123536, nmse 0.6557168102856166, rmse 0.26188659379002843, mae 0.20729368529132183, rsquared -0.7876441659136237\n",
      "train epoch 95, loss 2.934600188864147, nmse 0.011246850478117073, rmse 0.04453809380274837, mae 0.009066510259191127, rsquared 0.9626706239068639\n",
      "validation epoch 95, loss 11.52289538249267, nmse 0.6562017026712113, rmse 0.26198340642751833, mae 0.20736155614750565, rsquared -0.7889661009785898\n",
      "train epoch 96, loss 2.9330136245833778, nmse 0.011240771370216718, rmse 0.04452605538920392, mae 0.008934745877140996, rsquared 0.962690801049395\n",
      "validation epoch 96, loss 11.516635141296945, nmse 0.6558420909385893, rmse 0.261911610501496, mae 0.20734292490255826, rsquared -0.7879857115700342\n",
      "train epoch 97, loss 2.931755343307349, nmse 0.011235947871855593, rmse 0.044516501133847995, mae 0.00882471633705382, rsquared 0.962706810703365\n",
      "validation epoch 97, loss 11.520070015789894, nmse 0.656040363582087, rmse 0.261951197761795, mae 0.20738384942707258, rsquared -0.788526251218933\n",
      "train epoch 98, loss 2.931079863447195, nmse 0.0112333593936853, rmse 0.044511373100592576, mae 0.008769802299290546, rsquared 0.9627154021108275\n",
      "validation epoch 98, loss 11.516470734456043, nmse 0.6558334005510948, rmse 0.26190987523552567, mae 0.20736517180364106, rsquared -0.7879620194513324\n",
      "train epoch 99, loss 2.9309649373353768, nmse 0.011232918000757961, rmse 0.04451049859824712, mae 0.008761356942494504, rsquared 0.9627168671363137\n",
      "validation epoch 99, loss 11.512003419098534, nmse 0.6555785613571669, rmse 0.2618589847300189, mae 0.20734885122527125, rsquared -0.7872672655711126\n"
     ]
    }
   ],
   "source": [
    "from src.dl.loader import BlockRegressDataset\n",
    "from src.dl.resnet import RegressResNet3d\n",
    "from sklearn.model_selection import KFold\n",
    "import collections\n",
    "train_set = BlockRegressDataset('train')\n",
    "val_set = BlockRegressDataset('validate')\n",
    "test_set = BlockRegressDataset('test')\n",
    "fold_num = 5\n",
    "kf = KFold(n_splits=fold_num, shuffle=True, random_state=10)\n",
    "best_models = np.empty(dtype=collections.OrderedDict, shape=fold_num)\n",
    "net = RegressResNet3d().cuda()\n",
    "loss_fn = nn.SmoothL1Loss().cuda()\n",
    "#lr = 1e-2\n",
    "#optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "lr = 1e-3\n",
    "#optim = torch.optim.SGD([{'params': net.fc.weight, 'lr': 1e-2}], lr=lr)\n",
    "optim = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "epoch = 100\n",
    "train_loader = DataLoader(train_set, batch_size=8)\n",
    "val_loader = DataLoader(val_set, batch_size=8)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    predy = np.array([])\n",
    "    y = np.array([])\n",
    "    net.train()\n",
    "    for step, [img, labels, score] in enumerate(train_loader):\n",
    "        img = img.cuda()\n",
    "        labels = labels.cuda()\n",
    "        score = score.cuda()\n",
    "        output = net(img, labels).squeeze(-1)\n",
    "        loss = loss_fn(output, score)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        output = output.cpu().detach().numpy()\n",
    "        score = score.cpu().detach().numpy()\n",
    "        predy = np.concatenate((predy, output), axis=0)\n",
    "        y = np.concatenate((y, score), axis=0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(train_set)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(train_set)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = r2_score(y, predy)\n",
    "    print('train epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))\n",
    "    \n",
    "    if log:\n",
    "        writer.add_scalar('train/Loss', total_loss, i)\n",
    "        writer.add_scalar('train/NMSE', nmse, i)\n",
    "        writer.add_scalar('train/RMSE', rmse, i)\n",
    "        writer.add_scalar('train/MAE', mae, i)\n",
    "        writer.add_scalar('train/rsquared', rsquared, i)\n",
    "        \n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    predy = np.array([])\n",
    "    y = np.array([])\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for step, [img, labels, score] in enumerate(val_loader):\n",
    "            img = img.cuda()\n",
    "            labels = labels.cuda()\n",
    "            score = score.cuda()\n",
    "            output = net(img, labels).squeeze(-1)\n",
    "            loss = loss_fn(output, score)\n",
    "\n",
    "            output = output.cpu().detach().numpy()\n",
    "            score = score.cpu().detach().numpy()\n",
    "            #print(\"y: \", score)\n",
    "            #print(\"y_pred: \", output)\n",
    "            predy = np.concatenate((predy, output), axis=0)\n",
    "            y = np.concatenate((y, score), axis=0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(val_set)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(val_set)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = r2_score(y, predy)\n",
    "    print('validation epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))\n",
    "    if log:\n",
    "        writer.add_scalar('validation/Loss', total_loss, i)\n",
    "        writer.add_scalar('validation/NMSE', nmse, i)\n",
    "        writer.add_scalar('validation/RMSE', rmse, i)\n",
    "        writer.add_scalar('validation/MAE', mae, i)\n",
    "        writer.add_scalar('validation/rsquared', rsquared, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss 14.81796445565536, nmse 0.38246806504802155, rmse 0.2794655588970451, mae 0.2638362386154233, rsquared -0.5403551487274227\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for step, [img, labels, score] in enumerate(test_loader):\n",
    "        img = img.cuda()\n",
    "        labels = labels.cuda()\n",
    "        score = score.cuda()\n",
    "        #score = torch.Tensor(np.random.randint(0, 3, score.size())).type(torch.LongTensor).cuda()\n",
    "        output = net(img, labels).squeeze(-1)\n",
    "        loss = loss_fn(output, score)\n",
    "        \n",
    "        predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "        y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "        total_loss += loss.item()\n",
    "nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "mse = np.sum((y - predy) ** 2) / len(test_set)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.sum(np.abs(y - predy)) / len(test_set)\n",
    "meany = np.mean(y)\n",
    "rsquared = 1 - np.sum((predy - y) ** 2) / np.sum((y - meany) ** 2)\n",
    "print('epoch {}, loss {}, nmse {}, rmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, rmse, mae, rsquared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss().cuda()\n",
    "lr = 1e-3\n",
    "optim = torch.optim.SGD([\n",
    "    {'params': net.fc.weight, 'lr': 1e-2}\n",
    "    ], lr=lr, momentum=0.3)\n",
    "#optim = torch.optim.SGD(net.parameters(), lr=1e-1, momentum=0.3)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.99)\n",
    "#optim = torch.optim.Adam(net.parameters(), lr=lr, betas=[0.3, 0.1])\n",
    "epoch = 100\n",
    "dataset = BlockRegressDataset('train')\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "    train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = sampler.SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
    "    for i in range(epoch):\n",
    "        total_loss = 0\n",
    "        predy = np.array([])\n",
    "        y = np.array([])\n",
    "        net.train()\n",
    "        for step, [img, labels, score] in enumerate(train_loader):\n",
    "            img = img.cuda()\n",
    "            labels = labels.cuda()\n",
    "            score = score.cuda()\n",
    "            output = net(img, labels).squeeze(-1)\n",
    "            loss = loss_fn(output.float(), score.float())\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            #scheduler.step()\n",
    "        \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, [img, labels, score] in enumerate(val_loader):\n",
    "                img = img.cuda()\n",
    "                labels = labels.cuda()\n",
    "                score = score.cuda()\n",
    "                output = net(img, labels).squeeze(-1)\n",
    "                loss = loss_fn(output.float(), score.float())\n",
    "        \n",
    "                predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "                y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "        mse = np.sum((y - predy) ** 2) / len(train_idx)\n",
    "        mae = np.sum(np.abs(y - predy)) / len(train_idx)\n",
    "        meany = np.mean(y)\n",
    "        rsquared = np.sum((predy - meany) ** 2) / np.sum((y - meany) ** 2)\n",
    "        print('epoch {}, loss {}, nmse {}, mae {}, rsquared {}'.format(i, total_loss, nmse, mae, rsquared))\n",
    "        writer.add_scalar('Loss', total_loss, i)\n",
    "        writer.add_scalar('NMSE', nmse, i)\n",
    "        writer.add_scalar('MAE', mae, i)\n",
    "        writer.add_scalar('rsquared', rsquared, i)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predy = []\n",
    "y = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "    train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = sampler.SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, [img, labels, score] in enumerate(train_loader):\n",
    "            img = img.cuda()\n",
    "            labels = labels.cuda()\n",
    "            score = score.cuda()\n",
    "            output = net(img, labels).squeeze(-1)\n",
    "            loss = loss_fn(output.float(), score.float())\n",
    "    \n",
    "            predy = np.concatenate((predy, output.cpu().detach().numpy()), axis=0)\n",
    "            y = np.concatenate((y, score.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    nmse = np.sum((y - predy) ** 2) / np.sum(y ** 2)\n",
    "    mse = np.sum((y - predy) ** 2) / len(train_idx)\n",
    "    mae = np.sum(np.abs(y - predy)) / len(train_idx)\n",
    "    meany = np.mean(y)\n",
    "    rsquared = np.sum((predy - meany) ** 2) / np.sum((y - meany) ** 2)\n",
    "    print(rsquared)\n",
    "    print(mse)\n",
    "    print(mae)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "420353a47962bc0cada1a6173771095a3d05bd8a3ecc61a5b633bf029926f1ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
